{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Model applied to Lac Operon simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model used is provided in pyro examples https://pyro.ai/examples/dmm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea here is that we have observations of variables at certain steps (say every 100 steps of the stochastic sim). Can we train a model for this sequential data, with a latent variable to describe the interaction between variables. Could this account for issues like in Lac Operon like cycles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../images/model_example.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_{123}, z_{123}) = p(x_1|z_1)p(x_2|z_2)p(x_3|z_3)p(z_3|z_2)P(z_2|z_1)p(z_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random\n",
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "# from torch import nn, tensor\n",
    "# import pyro\n",
    "# from pyro.optim import ClippedAdam\n",
    "# from pyro.infer import SVI, Trace_ELBO\n",
    "# import pyro.distributions as dist\n",
    "# import pyro.poutine as poutine\n",
    "# from pyro.contrib.examples import polyphonic_data_loader as poly\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../data/sim_data.pickle\", \"rb\") as input_file:\n",
    "     sim_data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rna_M</th>\n",
       "      <th>monomer_betaGal</th>\n",
       "      <th>protein_betaGal</th>\n",
       "      <th>Lactose_external</th>\n",
       "      <th>Lactose_internal</th>\n",
       "      <th>Glucose_external</th>\n",
       "      <th>Glucose_internal</th>\n",
       "      <th>protein_Lactose_Permease</th>\n",
       "      <th>dna_Lac_Operon</th>\n",
       "      <th>Biomass</th>\n",
       "      <th>Lactose_consumed</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120440000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60220000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120440000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59918175.0</td>\n",
       "      <td>301825.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120440000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59617267.0</td>\n",
       "      <td>602733.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120440000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59315263.0</td>\n",
       "      <td>904737.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120440000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59014271.0</td>\n",
       "      <td>1205729.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rna_M  monomer_betaGal  protein_betaGal  Lactose_external  \\\n",
       "0    0.0              0.0              0.0       120440000.0   \n",
       "1    0.0              0.0              0.0       120440000.0   \n",
       "2    0.0              0.0              0.0       120440000.0   \n",
       "3    0.0              0.0              0.0       120440000.0   \n",
       "4    0.0              0.0              0.0       120440000.0   \n",
       "\n",
       "   Lactose_internal  Glucose_external  Glucose_internal  \\\n",
       "0               0.0        60220000.0               0.0   \n",
       "1               0.0        59918175.0          301825.0   \n",
       "2               0.0        59617267.0          602733.0   \n",
       "3               0.0        59315263.0          904737.0   \n",
       "4               0.0        59014271.0         1205729.0   \n",
       "\n",
       "   protein_Lactose_Permease  dna_Lac_Operon  Biomass  Lactose_consumed  time  \n",
       "0                       0.0             2.0   1000.0               0.0   0.0  \n",
       "1                       0.0             2.0   1000.0               0.0   1.0  \n",
       "2                       0.0             2.0   1000.0               0.0   2.0  \n",
       "3                       0.0             2.0   1000.0               0.0   3.0  \n",
       "4                       0.0             2.0   1000.0               0.0   4.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_data[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sim_data[0].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition from latent Z to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emitter(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the linear observation likelihood p(x_t | z_t)\n",
    "    \n",
    "    Devon: bernoulli is wrong for this model I think\n",
    "            Added linear observation. I think theoretically we might be able to use a better output function\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, z_dim, emission_dim):\n",
    "        super().__init__()\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)\n",
    "        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)\n",
    "        self.lin_hidden_to_input = nn.Linear(emission_dim, input_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid() ## We want the resulting value to be continuous\n",
    "        self.linear = nn.Linear(input_dim, input_dim) ## Assuming internal glucose is output\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z_t):\n",
    "        \"\"\"\n",
    "        Given the latent z at a particular time step t we return the vector of\n",
    "        probabilities `ps` that parameterizes the bernoulli distribution p(x_t|z_t)\n",
    "        \"\"\"\n",
    "#         h1 = self.relu(self.lin_z_to_hidden(z_t))\n",
    "#         h2 = self.relu(self.lin_hidden_to_hidden(h1))\n",
    "#         #ps = self.linear(self.lin_hidden_to_input(h2))\n",
    "#         ps = self.sigmoid(self.lin_hidden_to_input(h2))\n",
    "\n",
    "        # compute the gating function\n",
    "        _gate = self.relu(self.lin_z_to_hidden(z_t))\n",
    "        gate = self.sigmoid(self.lin_hidden_to_input(_gate))\n",
    "        # compute the 'proposed mean'\n",
    "        _proposed_mean = self.relu(self.lin_z_to_hidden(z_t))\n",
    "        proposed_mean = self.lin_hidden_to_input(_proposed_mean)\n",
    "        # assemble the actual mean used to sample z_t, which mixes\n",
    "        # a linear transformation of z_{t-1} with the proposed mean\n",
    "        # modulated by the gating function\n",
    "        loc = (1 - gate) * self.lin_hidden_to_input(z_t) + gate * proposed_mean\n",
    "        # compute the scale used to sample z_t, using the proposed\n",
    "        # mean from above as input. the softplus ensures that scale is positive\n",
    "        scale = self.softplus(self.linear(self.relu(proposed_mean)))\n",
    "        \n",
    "        return loc, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition between latent Zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedTransition(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the gaussian latent transition probability p(z_t | z_{t-1})\n",
    "    See section 5 in the reference for comparison.\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, transition_dim):\n",
    "        super().__init__()\n",
    "        # initialize the six linear transformations used in the neural network\n",
    "        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_sig = nn.Linear(z_dim, z_dim)\n",
    "        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)\n",
    "        # modify the default initialization of lin_z_to_loc\n",
    "        # so that it's starts out as the identity function\n",
    "        self.lin_z_to_loc.weight.data = torch.eye(z_dim)\n",
    "        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)\n",
    "        # initialize the three non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z_t_1):\n",
    "        \"\"\"\n",
    "        Given the latent z_{t-1} corresponding to the time step t-1\n",
    "        we return the mean and scale vectors that parameterize the\n",
    "        (diagonal) gaussian distribution p(z_t | z_{t-1})\n",
    "        \"\"\"\n",
    "        # compute the gating function\n",
    "        _gate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\n",
    "        gate = self.sigmoid(self.lin_gate_hidden_to_z(_gate))\n",
    "        # compute the 'proposed mean'\n",
    "        _proposed_mean = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\n",
    "        proposed_mean = self.lin_proposed_mean_hidden_to_z(_proposed_mean)\n",
    "        # assemble the actual mean used to sample z_t, which mixes\n",
    "        # a linear transformation of z_{t-1} with the proposed mean\n",
    "        # modulated by the gating function\n",
    "        loc = (1 - gate) * self.lin_z_to_loc(z_t_1) + gate * proposed_mean\n",
    "        # compute the scale used to sample z_t, using the proposed\n",
    "        # mean from above as input. the softplus ensures that scale is positive\n",
    "        scale = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n",
    "        # return loc, scale which can be fed into Normal\n",
    "        return loc, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combiner(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes q(z_t | z_{t-1}, x_{t:T}), which is the basic building block\n",
    "    of the guide (i.e. the variational distribution). The dependence on x_{t:T} is\n",
    "    through the hidden state of the RNN (see the pytorch module `rnn` below)\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, rnn_dim):\n",
    "        super().__init__()\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)\n",
    "        self.lin_hidden_to_loc = nn.Linear(rnn_dim, z_dim)\n",
    "        self.lin_hidden_to_scale = nn.Linear(rnn_dim, z_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "        #self.softplus = nn.ReLU()\n",
    "\n",
    "    def forward(self, z_t_1, h_rnn):\n",
    "        \"\"\"\n",
    "        Given the latent z at at a particular time step t-1 as well as the hidden\n",
    "        state of the RNN h(x_{t:T}) we return the mean and scale vectors that\n",
    "        parameterize the (diagonal) gaussian distribution q(z_t | z_{t-1}, x_{t:T})\n",
    "        \"\"\"\n",
    "        # combine the rnn hidden state with a transformed version of z_t_1\n",
    "        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)\n",
    "        # use the combined hidden state to compute the mean used to sample z_t\n",
    "        loc = self.lin_hidden_to_loc(h_combined)\n",
    "        # use the combined hidden state to compute the scale used to sample z_t\n",
    "        scale = self.softplus(self.lin_hidden_to_scale(h_combined))\n",
    "        #scale = self.softplus(h_combined)\n",
    "\n",
    "        # return loc, scale which can be fed into Normal\n",
    "        return loc, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMM(nn.Module):\n",
    "    \"\"\"\n",
    "    This PyTorch Module encapsulates the model as well as the\n",
    "    variational distribution (the guide) for the Deep Markov Model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=12, z_dim=25, emission_dim=25,\n",
    "                 transition_dim=50, rnn_dim=150, rnn_dropout_rate=0.0,\n",
    "                 num_iafs=0, iaf_dim=50, use_cuda=True):\n",
    "        super().__init__()\n",
    "        # instantiate pytorch modules used in the model and guide below\n",
    "        self.emitter = Emitter(input_dim, z_dim, emission_dim)\n",
    "        self.trans = GatedTransition(z_dim, transition_dim)\n",
    "        self.combiner = Combiner(z_dim, rnn_dim)\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=rnn_dim,\n",
    "                          nonlinearity='relu', batch_first=True,\n",
    "                          bidirectional=False, num_layers=1, dropout=rnn_dropout_rate)\n",
    "\n",
    "        # define a (trainable) parameters z_0 and z_q_0 that help define\n",
    "        # the probability distributions p(z_1) and q(z_1)\n",
    "        # (since for t = 1 there are no previous latents to condition on)\n",
    "        self.z_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        self.z_q_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        # define a (trainable) parameter for the initial hidden state of the rnn\n",
    "        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        # if on gpu cuda-ize all pytorch (sub)modules\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    # the model p(x_{1:T} | z_{1:T}) p(z_{1:T})\n",
    "    def model(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "              mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "\n",
    "        # register all PyTorch (sub)modules with pyro\n",
    "        # this needs to happen in both the model and guide\n",
    "        pyro.module(\"dmm\", self)\n",
    "\n",
    "        # set z_prev = z_0 to setup the recursive conditioning in p(z_t | z_{t-1})\n",
    "        z_prev = self.z_0.expand(mini_batch.size(0), self.z_0.size(0))\n",
    "\n",
    "        # we enclose all the sample statements in the model in a plate.\n",
    "        # this marks that each datapoint is conditionally independent of the others\n",
    "        with pyro.plate(\"z_minibatch\", len(mini_batch)):\n",
    "            # sample the latents z and observed x's one time step at a time\n",
    "            for t in range(1, T_max + 1):\n",
    "                # the next chunk of code samples z_t ~ p(z_t | z_{t-1})\n",
    "                # note that (both here and elsewhere) we use poutine.scale to take care\n",
    "                # of KL annealing. we use the mask() method to deal with raggedness\n",
    "                # in the observed data (i.e. different sequences in the mini-batch\n",
    "                # have different lengths)\n",
    "\n",
    "                # first compute the parameters of the diagonal gaussian\n",
    "                # distribution p(z_t | z_{t-1})\n",
    "                z_loc, z_scale = self.trans(z_prev)\n",
    "\n",
    "                # then sample z_t according to dist.Normal(z_loc, z_scale).\n",
    "                # note that we use the reshape method so that the univariate\n",
    "                # Normal distribution is treated as a multivariate Normal\n",
    "                # distribution with a diagonal covariance.\n",
    "                with poutine.scale(None, annealing_factor):\n",
    "                    z_t = pyro.sample(\"z_%d\" % t,\n",
    "                                      dist.Normal(z_loc, z_scale)\n",
    "                                          .mask(mini_batch_mask[:, t - 1:t])\n",
    "                                          .to_event(1))\n",
    "\n",
    "                # compute the probabilities that parameterize the bernoulli likelihood\n",
    "                emission_probs_mean, emission_probs_scale = self.emitter(z_t)\n",
    "                # the next statement instructs pyro to observe x_t according to the\n",
    "                # bernoulli distribution p(x_t|z_t)\n",
    "                pyro.sample(\"obs_x_%d\" % t,\n",
    "                            #dist.Bernoulli(emission_probs_t)\n",
    "                            dist.Normal(emission_probs_mean, emission_probs_scale)\n",
    "                                .mask(mini_batch_mask[:, t - 1:t])\n",
    "                                .to_event(1),\n",
    "                            obs=mini_batch[:, t - 1, :])\n",
    "                # the latent sampled at this time step will be conditioned upon\n",
    "                # in the next time step so keep track of it\n",
    "                z_prev = z_t\n",
    "\n",
    "    # the guide q(z_{1:T} | x_{1:T}) (i.e. the variational distribution)\n",
    "    def guide(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "          mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "        # register all PyTorch (sub)modules with pyro\n",
    "        pyro.module(\"dmm\", self)\n",
    "\n",
    "        # if on gpu we need the fully broadcast view of the rnn initial state\n",
    "        # to be in contiguous gpu memory\n",
    "        h_0_contig = self.h_0.expand(1, mini_batch.size(0),\n",
    "                                     self.rnn.hidden_size).contiguous()\n",
    "        # push the observed x's through the rnn;\n",
    "        # rnn_output contains the hidden state at each time step\n",
    "        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
    "        # reverse the time-ordering in the hidden state and un-pack it\n",
    "        rnn_output = pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n",
    "        # set z_prev = z_q_0 to setup the recursive conditioning in q(z_t |...)\n",
    "        z_prev = self.z_q_0.expand(mini_batch.size(0), self.z_q_0.size(0))\n",
    "\n",
    "        # we enclose all the sample statements in the guide in a plate.\n",
    "        # this marks that each datapoint is conditionally independent of the others.\n",
    "        with pyro.plate(\"z_minibatch\", len(mini_batch)):\n",
    "            # sample the latents z one time step at a time\n",
    "            for t in range(1, T_max + 1):\n",
    "                # the next two lines assemble the distribution q(z_t | z_{t-1}, x_{t:T})\n",
    "                z_loc, z_scale = self.combiner(z_prev, rnn_output[:, t - 1, :])\n",
    "                \n",
    "                z_dist = dist.Normal(tensor(z_loc), tensor(z_scale))\n",
    "\n",
    "                # sample z_t from the distribution z_dist\n",
    "                with pyro.poutine.scale(None, annealing_factor):\n",
    "                    z_t = pyro.sample(\"z_%d\" % t,\n",
    "                                      z_dist.mask(mini_batch_mask[:, t - 1:t])\n",
    "                                            .to_event(1))\n",
    "                # the latent sampled at this time step will be conditioned\n",
    "                # upon in the next time step so keep track of it\n",
    "                z_prev = z_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyro.contrib.examples.polyphonic_data_loader Functions\n",
    "\n",
    "This pyro function is not working in current version of pyro. Maybe would work if directly installed dev branch from github. Just load the functions here instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the hidden state as output by the PyTorch rnn and\n",
    "# unpacks it it; it also reverses each sequence temporally\n",
    "def pad_and_reverse(rnn_output, seq_lengths):\n",
    "    rnn_output, _ = nn.utils.rnn.pad_packed_sequence(rnn_output, batch_first=True)\n",
    "    reversed_output = reverse_sequences(rnn_output, seq_lengths)\n",
    "    return reversed_output\n",
    "def reverse_sequences(mini_batch, seq_lengths):\n",
    "    reversed_mini_batch = torch.zeros_like(mini_batch)\n",
    "    for b in range(mini_batch.size(0)):\n",
    "        T = seq_lengths[b]\n",
    "        time_slice = torch.arange(T - 1, -1, -1, device=mini_batch.device)\n",
    "        reversed_sequence = torch.index_select(mini_batch[b, :, :], 0, time_slice)\n",
    "        reversed_mini_batch[b, 0:T, :] = reversed_sequence\n",
    "    return reversed_mini_batch\n",
    "# this function returns a 0/1 mask that can be used to mask out a mini-batch\n",
    "# composed of sequences of length `seq_lengths`\n",
    "def get_mini_batch_mask(mini_batch, seq_lengths):\n",
    "    mask = torch.zeros(mini_batch.shape[0:2])\n",
    "    for b in range(mini_batch.shape[0]):\n",
    "        mask[b, 0 : seq_lengths[b]] = torch.ones(seq_lengths[b])\n",
    "    return mask\n",
    "# this function prepares a mini-batch for training or evaluation.\n",
    "# it returns a mini-batch in forward temporal order (`mini_batch`) as\n",
    "# well as a mini-batch in reverse temporal order (`mini_batch_reversed`).\n",
    "# it also deals with the fact that packed sequences (which are what what we\n",
    "# feed to the PyTorch rnn) need to be sorted by sequence length.\n",
    "def get_mini_batch(mini_batch_indices, sequences, seq_lengths, cuda=True):\n",
    "    # get the sequence lengths of the mini-batch\n",
    "    seq_lengths = seq_lengths[mini_batch_indices]\n",
    "    # sort the sequence lengths\n",
    "    _, sorted_seq_length_indices = torch.sort(seq_lengths)\n",
    "    sorted_seq_length_indices = sorted_seq_length_indices.flip(0)\n",
    "    sorted_seq_lengths = seq_lengths[sorted_seq_length_indices]\n",
    "    sorted_mini_batch_indices = mini_batch_indices[sorted_seq_length_indices]\n",
    "\n",
    "    # compute the length of the longest sequence in the mini-batch\n",
    "    T_max = torch.max(seq_lengths)\n",
    "    # this is the sorted mini-batch\n",
    "    mini_batch = sequences[sorted_mini_batch_indices, 0:T_max, :]\n",
    "    # this is the sorted mini-batch in reverse temporal order\n",
    "    mini_batch_reversed = reverse_sequences(mini_batch, sorted_seq_lengths)\n",
    "    # get mask for mini-batch\n",
    "    mini_batch_mask = get_mini_batch_mask(mini_batch, sorted_seq_lengths)\n",
    "\n",
    "    # cuda() here because need to cuda() before packing\n",
    "    if cuda:\n",
    "        mini_batch = mini_batch.cuda()\n",
    "        mini_batch_mask = mini_batch_mask.cuda()\n",
    "        mini_batch_reversed = mini_batch_reversed.cuda()\n",
    "\n",
    "    # do sequence packing\n",
    "    mini_batch_reversed = nn.utils.rnn.pack_padded_sequence(\n",
    "        mini_batch_reversed, sorted_seq_lengths, batch_first=True\n",
    "    )\n",
    "\n",
    "    return mini_batch, mini_batch_reversed, mini_batch_mask, sorted_seq_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the dmm\n",
    "dmm = DMM(input_dim=9, z_dim=25, emission_dim=25,\n",
    "                 transition_dim=50, rnn_dim=150, rnn_dropout_rate=0.0,\n",
    "                 num_iafs=0, iaf_dim=50, use_cuda=False)##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "adam_params = {\"lr\": .01, \"betas\": (.9, .999),\n",
    "               \"clip_norm\": 1., \"lrd\": 0.001,\n",
    "               \"weight_decay\": 0.001}\n",
    "optimizer = ClippedAdam(adam_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(dmm.model, dmm.guide, optimizer, Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_samples = [sim_data[i].iloc[::100, :].drop('time', axis = 1) for i in range(len(sim_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim_data_samples = [sim_data_samples[i].replace(to_replace=0, value=.1) for i in range(len(sim_data_samples))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2)\n",
    "train_idx = random.sample(range(len(sim_data_samples)), int(len(sim_data_samples) * .8))\n",
    "val_test_idx = [x for x in range(len(sim_data_samples)) if x not in train_idx]\n",
    "val_idx = random.sample(val_test_idx, int(len(val_test_idx) * .5))\n",
    "test_idx = [x for x in val_test_idx if x not in val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [sim_data_samples[i] for i in train_idx]\n",
    "val_data = [sim_data_samples[i] for i in val_idx]\n",
    "test_data = [sim_data_samples[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat(train_data)\n",
    "train_mean = combined.mean(axis = 0)\n",
    "train_std = combined.std(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)): \n",
    "    for col in range(len(train_data[i].columns)):\n",
    "        train_data[i].iloc[:,col] = (train_data[i].iloc[:,col] - train_mean[col])/train_std[col]\n",
    "    \n",
    "    ## Data simulated without variation in dna_Lac_Operon or Biomass. Just drop them temporarily\n",
    "    train_data[i] = train_data[i].drop(columns=['dna_Lac_Operon', 'Biomass'])\n",
    "\n",
    "## Bad code fix later\n",
    "for i in range(len(val_data)): \n",
    "    for col in range(len(val_data[i].columns)):\n",
    "        val_data[i].iloc[:,col] = (val_data[i].iloc[:,col] - train_mean[col])/train_std[col]\n",
    "    \n",
    "    ## Data simulated without variation in dna_Lac_Operon or Biomass. Just drop them temporarily\n",
    "    val_data[i] = val_data[i].drop(columns=['dna_Lac_Operon', 'Biomass'])\n",
    "    \n",
    "for i in range(len(test_data)): \n",
    "    for col in range(len(test_data[i].columns)):\n",
    "        test_data[i].iloc[:,col] = (test_data[i].iloc[:,col] - train_mean[col])/train_std[col]\n",
    "    \n",
    "    ## Data simulated without variation in dna_Lac_Operon or Biomass. Just drop them temporarily\n",
    "    test_data[i] = test_data[i].drop(columns=['dna_Lac_Operon', 'Biomass'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQAklEQVR4nO3deYxdZ33G8e8TxxD22PXY2ATiCizEIiVUJqSiLbQh1ARa5w9Qg9rgVkEWVVGJFKm4AYVFFFmqKHSBVmkIccvShrLEZTcuKWnFNg4JkCbUUQhJamNPNkIgBQy//nGP8XSY8dyZuffOvJ7vR7o66z33d84cP/P6PfecSVUhSWrPSYtdgCRpfgxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJoiyaVJrljsOvqR5I1J3rvYdWhxGOAaiCS3J3nBgLb1/CR3DWJb81FVb62qV/azrgGqxWSAS4soycmLXYPaZYBraJKsSvKxJBNJ7uvGT5u0fHWS9yQ50C3/aJJHAZ8ENiR5sHttSPLwJO/o1j3QjT+8286abtv3J7k3yXVJTuqWbUjyoa6GbyX54z7q/lmrOsnGJJVkW5I7ktyd5HXdsi3ApcDvdHXe2M1/XJJ3JzmY5H+SvCXJim7Z7yf5zyRvT3Iv8MYkVyV5Z5KPJ/leki8lefKkev4yyZ1JHkiyL8mvDuhHpMYZ4Bqmk4D3AKcDTwIeAv5m0vJ/BB4JPANYC7y9qr4PvAg4UFWP7l4HgNcBZwNnAmcAZwGv77ZzCXAXMAasoxeq1YX4vwI3Ak8AzgEuTvKb89iXXwGe2m3jsiRPq6pPAW8F/rmr84xu3V3AEeApwLOAFwKTu2SeA9zW7fOfdfNeDrwJWAXcOmk+wFe6/V4NvB/4YJJT5rEPOsEY4Bqaqrqnqj5UVT+oqu/RC6XnASRZTy+oX1VV91XVj6vq34+zud8F3lxVh6tqgl7YXdgt+zGwHji928511XtK27OBsap6c1X9qKpuA/4euGAeu/Omqnqoqm6k9wvhjOlWSrKu26+Lq+r7VXUYePuUzzxQVX9dVUeq6qFu3oer6stVdQR4H73ABqCq3tsdyyNV9Tbg4fR+mWiZs/9NQ5PkkfTCawu9liXAY7ruhCcC91bVfX1ubgPw7UnT3+7mAfw58EbgM0kALq+qnfRa/huS3D/pfSuA6+a8M/CdSeM/AB49w3qnAyuBg10t0Gso3TlpnTunvul4209yCb0W/AaggMcCa+ZQu05QBriG6RJ6LcXnVNV3kpwJfBUIvRBbneTUqrp/yvume8bxAXrheFM3/aRuHl3r/hLgkiTPAD6X5CvdZ3yrqjYNdK+OX+udwA+BNV1rup/3zKjr734tva6bm6rqp0nuo3cMtczZhaJBWpnklKMveq3uh4D7k6wG3nB0xao6SO9i5bu6i50rk/xat/gQ8AtJHjdp2x8AXp9kLMka4DLg6IXGlyR5SnpN3geAn3SvLwMPJHltkkckWZHkmUmePcB9PgRsPHrRtNuvzwBvS/LYJCcleXKS581z+4+h158+AZyc5DJ6LXDJANdAfYJeYB99nQo8Argb+CLwqSnrX0iv//oW4DBwMUBV3UIvsG/rvlmyAXgLMA58Dfg6cH03D2AT8FngQeALwLuq6tqq+gnwW/T6k7/V1XEFMPkXw0J9sBvek+T6bvwVwMOA/wLuA/6FXh/9fHya3i+6/6bXbfS/TN8Fo2Uo/kUeSWqTLXBJapQBrmUpyScn3Sg0+XXpYtcm9csuFElq1Ei/RrhmzZrauHHjKD9Skpq3b9++u6tqbOr8kQb4xo0bGR8fH+VHSlLzknx7uvn2gUtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqOa+Ys8G3d8/Gfjt+988SJWIklLgy1wSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY3qO8CTrEjy1SQf66ZXJ9mTZH83XDW8MiVJU82lBf4a4OZJ0zuAvVW1CdjbTUuSRqSvAE9yGvBi4IpJs7cCu7rxXcD5A61MknRc/bbA3wH8CfDTSfPWVdVBgG64dro3JtmeZDzJ+MTExEJqlSRNMmuAJ3kJcLiq9s3nA6rq8qraXFWbx8bG5rMJSdI0+nke+HOB305yHnAK8Ngk7wUOJVlfVQeTrAcOD7NQSdL/N2sLvKr+tKpOq6qNwAXAv1XV7wG7gW3datuAa4ZWpSTp5yzke+A7gXOT7AfO7aYlSSMypz+pVlXXAtd24/cA5wy+JElSP7wTU5IaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEbNGuBJTkny5SQ3JrkpyZu6+auT7EmyvxuuGn65kqSj+mmB/xD4jao6AzgT2JLkbGAHsLeqNgF7u2lJ0ojMGuDV82A3ubJ7FbAV2NXN3wWcP4wCJUnT66sPPMmKJDcAh4E9VfUlYF1VHQTohmtneO/2JONJxicmJgZUtiSprwCvqp9U1ZnAacBZSZ7Z7wdU1eVVtbmqNo+Njc2zTEnSVHP6FkpV3Q9cC2wBDiVZD9ANDw+6OEnSzPr5FspYklO78UcALwBuAXYD27rVtgHXDKlGSdI0Tu5jnfXAriQr6AX+1VX1sSRfAK5OchFwB/CyIdYpSZpi1gCvqq8Bz5pm/j3AOcMoSpI0O+/ElKRGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVGzBniSJyb5XJKbk9yU5DXd/NVJ9iTZ3w1XDb9cSdJR/bTAjwCXVNXTgLOBP0rydGAHsLeqNgF7u2lJ0ojMGuBVdbCqru/GvwfcDDwB2Ars6lbbBZw/pBolSdOYUx94ko3As4AvAeuq6iD0Qh5YO8N7ticZTzI+MTGxwHIlSUf1HeBJHg18CLi4qh7o931VdXlVba6qzWNjY/OpUZI0jb4CPMlKeuH9vqr6cDf7UJL13fL1wOHhlChJmk4/30IJ8G7g5qr6i0mLdgPbuvFtwDWDL0+SNJOT+1jnucCFwNeT3NDNuxTYCVyd5CLgDuBlQ6lQkjStWQO8qv4DyAyLzxlsOZKkfnknpiQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY2aNcCTXJnkcJJvTJq3OsmeJPu74arhlilJmqqfFvhVwJYp83YAe6tqE7C3m5YkjdCsAV5VnwfunTJ7K7CrG98FnD/YsiRJs5lvH/i6qjoI0A3XDq4kSVI/hn4RM8n2JONJxicmJob9cZK0bMw3wA8lWQ/QDQ/PtGJVXV5Vm6tq89jY2Dw/TpI01XwDfDewrRvfBlwzmHIkSf3q52uEHwC+ADw1yV1JLgJ2Aucm2Q+c201Lkkbo5NlWqKqXz7DonAHXIkmaA+/ElKRGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckho169/ElKTFsHHHx382fvvOFy9iJT1LrR6wBS5JzTLAJalRBrgkNcoAl6RGeRFT0tBMvvA32VwvAg5qOycaW+CS1CgDXJIaZYBLUqPsAxewuDcpDOqz57qdhazf73uWo5n6qxdLP/VM/ln2c17MdZ3jrbcQtsAlqVEGuCQ1ygCXpEYZ4JLUqOYvYs50MaGf+YPkBa1jhn1RcpQXKxf6cx3GBbFhn2uj+Kxh3+AzVzNtp5/tL+YXAGyBS1KjDHBJapQBLkmNMsAlqVGpqpF92ObNm2t8fHxe711qd3eNwiguvmow5vqzGtSdf/1cDJzrBcOl8sWAFh3vuCzkAmeSfVW1eep8W+CS1KgFBXiSLUm+meTWJDsGVZQkaXbzDvAkK4B3Ai8Cng68PMnTB1WYJOn4FtICPwu4tapuq6ofAf8EbB1MWZKk2cz7ImaSlwJbquqV3fSFwHOq6tVT1tsObO8mnwp8c/7lDsUa4O7FLmKJ8Fj0eByO8Vgcs5jH4vSqGps6cyG30meaeT/326CqLgcuX8DnDFWS8emu7i5HHosej8MxHotjluKxWEgXyl3AEydNnwYcWFg5kqR+LSTAvwJsSvKLSR4GXADsHkxZkqTZzLsLpaqOJHk18GlgBXBlVd00sMpGZ8l27ywCj0WPx+EYj8UxS+5YjPROTEnS4HgnpiQ1ygCXpEYtuwBPsjrJniT7u+GqGda7PcnXk9yQZH5P4FqCZnv8QXr+qlv+tSS/tBh1jkIfx+L5Sb7bnQM3JLlsMeoctiRXJjmc5BszLF9O58Rsx2JJnRPLLsCBHcDeqtoE7O2mZ/LrVXXmUvvu53z1+fiDFwGbutd24G9HWuSIzOFRENd158CZVfXmkRY5OlcBW46zfFmcE52rOP6xgCV0TizHAN8K7OrGdwHnL14pI9fP4w+2Av9QPV8ETk2yftSFjoCPguhU1eeBe4+zynI5J/o5FkvKcgzwdVV1EKAbrp1hvQI+k2Rf9ziAE8ETgDsnTd/VzZvrOieCfvfzl5PcmOSTSZ4xmtKWnOVyTvRryZwTzf9V+ukk+Szw+GkWvW4Om3luVR1IshbYk+SW7rdzy/p5/EFfj0g4AfSzn9fTewbFg0nOAz5KrxthuVku50Q/ltQ5cUK2wKvqBVX1zGle1wCHjv73rxsenmEbB7rhYeAj9P7L3bp+Hn+wXB6RMOt+VtUDVfVgN/4JYGWSNaMrcclYLufErJbaOXFCBvgsdgPbuvFtwDVTV0jyqCSPOToOvBCY9qp0Y/p5/MFu4BXdNw/OBr57tMvpBDPrsUjy+CTpxs+i9+/lnpFXuviWyzkxq6V2TpyQXSiz2AlcneQi4A7gZQBJNgBXVNV5wDrgI93P6WTg/VX1qUWqd2BmevxBkld1y/8O+ARwHnAr8APgDxar3mHq81i8FPjDJEeAh4AL6gS8dTnJB4DnA2uS3AW8AVgJy+ucgL6OxZI6J7yVXpIatRy7UCTphGCAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEb9H+A31AP8RRkPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in train_data[0].columns:\n",
    "    plt.hist(train_data[0].loc[:, col], bins = 100)\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seq_lengths = tensor(np.array([len(train_data[i]) for i in range(len(train_data))]))\n",
    "training_data_sequences = tensor(np.array([train_data[i].values for i in range(len(train_data))]))\n",
    "test_seq_lengths = tensor(np.array([len(val_data[i]) for i in range(len(val_data))]))\n",
    "test_data_sequences = tensor(np.array([val_data[i].values for i in range(len(val_data))]))\n",
    "val_seq_lengths = tensor(np.array([len(test_data[i]) for i in range(len(test_data))]))\n",
    "val_data_sequences = tensor(np.array([test_data[i].values for i in range(len(test_data))]))\n",
    "N_train_data = len(training_seq_lengths)\n",
    "N_train_time_slices = sum(training_seq_lengths)\n",
    "N_mini_batches = int(N_train_data / 10 + int(N_train_data % 10 > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(base_path, dataset, min_note=21, note_range=88):\n",
    "    output = os.path.join(base_path, dataset.filename)\n",
    "    if os.path.exists(output):\n",
    "        try:\n",
    "            with open(output, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        except (ValueError, UnicodeDecodeError):\n",
    "            # Assume python env has changed.\n",
    "            # Recreate pickle file in this env's format.\n",
    "            os.remove(output)\n",
    "\n",
    "    print(\"processing raw data - {} ...\".format(dataset.name))\n",
    "    data = pickle.load(urlopen(dataset.url))\n",
    "    processed_dataset = {}\n",
    "    for split, data_split in data.items():\n",
    "        processed_dataset[split] = {}\n",
    "        n_seqs = len(data_split)\n",
    "        processed_dataset[split][\"sequence_lengths\"] = torch.zeros(\n",
    "            n_seqs, dtype=torch.long\n",
    "        )\n",
    "        processed_dataset[split][\"sequences\"] = []\n",
    "        for seq in range(n_seqs):\n",
    "            seq_length = len(data_split[seq])\n",
    "            processed_dataset[split][\"sequence_lengths\"][seq] = seq_length\n",
    "            processed_sequence = torch.zeros((seq_length, note_range))\n",
    "            for t in range(seq_length):\n",
    "                note_slice = torch.tensor(list(data_split[seq][t])) - min_note\n",
    "                slice_length = len(note_slice)\n",
    "                if slice_length > 0:\n",
    "                    processed_sequence[t, note_slice] = torch.ones(slice_length)\n",
    "            processed_dataset[split][\"sequences\"].append(processed_sequence)\n",
    "    pickle.dump(processed_dataset, open(output, \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"dumped processed data to %s\" % output)\n",
    "# ingest training/validation/test data from disk\n",
    "def load_data(dataset):\n",
    "    # download and process dataset if it does not exist\n",
    "    base_path = r\"../data/\"\n",
    "    process_data(base_path, dataset)\n",
    "    file_loc = os.path.join(base_path, dataset.filename)\n",
    "    with open(file_loc, \"rb\") as f:\n",
    "        dset = pickle.load(f)\n",
    "        for k, v in dset.items():\n",
    "            sequences = v[\"sequences\"]\n",
    "            dset[k][\"sequences\"] = pad_sequence(sequences, batch_first=True).type(\n",
    "                torch.Tensor\n",
    "            )\n",
    "            dset[k][\"sequence_lengths\"] = v[\"sequence_lengths\"].to(\n",
    "                device=torch.Tensor().device\n",
    "            )\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = namedtuple(\"dset\", [\"name\", \"url\", \"filename\"])\n",
    "JSB_CHORALES = dset(\n",
    "    \"jsb_chorales\",\n",
    "    \"https://d2hg8soec8ck9v.cloudfront.net/datasets/polyphonic/jsb_chorales.pickle\",\n",
    "    \"jsb_chorales.pkl\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(JSB_CHORALES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test'][\"sequences\"][1][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batch(mini_batch_indices, sequences, seq_lengths, cuda=False):\n",
    "    # get the sequence lengths of the mini-batch\n",
    "    seq_lengths = seq_lengths[mini_batch_indices]\n",
    "    # sort the sequence lengths\n",
    "    _, sorted_seq_length_indices = torch.sort(seq_lengths)\n",
    "    sorted_seq_length_indices = sorted_seq_length_indices.flip(0)\n",
    "    sorted_seq_lengths = seq_lengths[sorted_seq_length_indices]\n",
    "    sorted_mini_batch_indices = mini_batch_indices[sorted_seq_length_indices]\n",
    "\n",
    "    # compute the length of the longest sequence in the mini-batch\n",
    "    T_max = torch.max(seq_lengths)\n",
    "    # this is the sorted mini-batch\n",
    "    mini_batch = sequences[sorted_mini_batch_indices, 0:T_max, :]\n",
    "    # this is the sorted mini-batch in reverse temporal order\n",
    "    mini_batch_reversed = reverse_sequences(mini_batch, sorted_seq_lengths)\n",
    "    # get mask for mini-batch\n",
    "    mini_batch_mask = get_mini_batch_mask(mini_batch, sorted_seq_lengths)\n",
    "\n",
    "    # cuda() here because need to cuda() before packing\n",
    "    if cuda:\n",
    "        mini_batch = mini_batch.cuda()\n",
    "        mini_batch_mask = mini_batch_mask.cuda()\n",
    "        mini_batch_reversed = mini_batch_reversed.cuda()\n",
    "\n",
    "    # do sequence packing\n",
    "    mini_batch_reversed = nn.utils.rnn.pack_padded_sequence(\n",
    "        mini_batch_reversed, sorted_seq_lengths, batch_first=True\n",
    "    )\n",
    "\n",
    "    return mini_batch, mini_batch_reversed, mini_batch_mask, sorted_seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_minibatch(epoch, which_mini_batch, shuffled_indices, mini_batch_size, annealing_epochs, minimum_annealing_factor):\n",
    "    if annealing_epochs > 0 and epoch < annealing_epochs:\n",
    "        # compute the KL annealing factor appropriate\n",
    "        # for the current mini-batch in the current epoch\n",
    "        min_af = minimum_annealing_factor\n",
    "        annealing_factor = min_af + (1.0 - min_af) * \\\n",
    "            (float(which_mini_batch + epoch * N_mini_batches + 1) /\n",
    "             float(annealing_epochs * N_mini_batches))\n",
    "    else:\n",
    "        # by default the KL annealing factor is unity\n",
    "        annealing_factor = 1.0\n",
    "\n",
    "    # compute which sequences in the training set we should grab\n",
    "    mini_batch_start = (which_mini_batch * mini_batch_size)\n",
    "    mini_batch_end = np.min([(which_mini_batch + 1) * mini_batch_size,\n",
    "                             N_train_data])\n",
    "    mini_batch_indices = shuffled_indices[mini_batch_start:mini_batch_end]\n",
    "    # grab the fully prepped mini-batch using the helper function in the data loader\n",
    "    mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths \\\n",
    "        = get_mini_batch(mini_batch_indices, training_data_sequences,\n",
    "                              training_seq_lengths, cuda=False)\n",
    "    mini_batch = mini_batch.float()\n",
    "    mini_batch_reversed = mini_batch_reversed.float()\n",
    "    mini_batch_mask = mini_batch_mask.float()\n",
    "    #mini_batch_seq_lengths = mini_batch_seq_lengths.float()\n",
    "\n",
    "    # do an actual gradient step\n",
    "    loss = svi.step(mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "                     mini_batch_seq_lengths, annealing_factor)\n",
    "    # keep track of the training loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup function to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package repeated copies of val/test data for faster evaluation\n",
    "# (i.e. set us up for vectorization)\n",
    "n_eval_samples = 1\n",
    "def rep(x):\n",
    "    return np.repeat(x, n_eval_samples, axis=0)\n",
    "\n",
    "# get the validation/test data ready for the dmm: pack into sequences, etc.\n",
    "val_seq_lengths = rep(val_seq_lengths)\n",
    "test_seq_lengths = rep(test_seq_lengths)\n",
    "val_batch, val_batch_reversed, val_batch_mask, val_seq_lengths = get_mini_batch(\n",
    "    np.arange(n_eval_samples * val_data_sequences.shape[0]), rep(val_data_sequences),\n",
    "    val_seq_lengths, cuda=False)\n",
    "val_batch = val_batch.float()\n",
    "val_batch_reversed = val_batch_reversed.float()\n",
    "val_batch_mask = val_batch_mask.float()\n",
    "\n",
    "test_batch, test_batch_reversed, test_batch_mask, test_seq_lengths = \\\n",
    "    get_mini_batch(np.arange(n_eval_samples * test_data_sequences.shape[0]),\n",
    "                        rep(test_data_sequences),\n",
    "                        test_seq_lengths, cuda=False)\n",
    "test_batch = test_batch.float()\n",
    "test_batch_reversed = test_batch_reversed.float()\n",
    "test_batch_mask = test_batch_mask.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2000, dtype=torch.int32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(val_seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_evaluation():\n",
    "    # put the RNN into evaluation mode (i.e. turn off drop-out if applicable)\n",
    "    dmm.rnn.eval()\n",
    "\n",
    "    # compute the validation and test loss\n",
    "    val_nll = svi.evaluate_loss(val_batch, val_batch_reversed, val_batch_mask,\n",
    "                                 val_seq_lengths) / sum(val_seq_lengths)\n",
    "    test_nll = svi.evaluate_loss(test_batch, test_batch_reversed, test_batch_mask,\n",
    "                                  test_seq_lengths) / sum(test_seq_lengths)\n",
    "\n",
    "    # put the RNN back into training mode (i.e. turn on drop-out if applicable)\n",
    "    dmm.rnn.train()\n",
    "    return val_nll, test_nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1500\n",
    "val_test_frequency = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-99292c118c6e>:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_dist = dist.Normal(tensor(z_loc), tensor(z_scale))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training epoch 0000]  15.8875 \t\t\t\t(dt = 4.310 sec)\n",
      "[training epoch 0001]  15.8979 \t\t\t\t(dt = 4.249 sec)\n",
      "[training epoch 0002]  15.9280 \t\t\t\t(dt = 4.296 sec)\n",
      "[training epoch 0003]  15.9738 \t\t\t\t(dt = 4.138 sec)\n",
      "[training epoch 0004]  16.0695 \t\t\t\t(dt = 4.124 sec)\n",
      "[training epoch 0005]  16.0518 \t\t\t\t(dt = 4.149 sec)\n",
      "[training epoch 0006]  16.0981 \t\t\t\t(dt = 4.114 sec)\n",
      "[training epoch 0007]  16.0981 \t\t\t\t(dt = 4.180 sec)\n",
      "[training epoch 0008]  16.1770 \t\t\t\t(dt = 4.126 sec)\n",
      "[training epoch 0009]  16.2353 \t\t\t\t(dt = 4.113 sec)\n",
      "[training epoch 0010]  16.2747 \t\t\t\t(dt = 4.158 sec)\n",
      "[training epoch 0011]  16.2704 \t\t\t\t(dt = 4.331 sec)\n",
      "[training epoch 0012]  16.3073 \t\t\t\t(dt = 4.079 sec)\n",
      "[training epoch 0013]  16.3764 \t\t\t\t(dt = 4.159 sec)\n",
      "[training epoch 0014]  16.4081 \t\t\t\t(dt = 4.139 sec)\n",
      "[training epoch 0015]  16.4800 \t\t\t\t(dt = 4.127 sec)\n",
      "[training epoch 0016]  16.5252 \t\t\t\t(dt = 4.154 sec)\n",
      "[training epoch 0017]  16.5696 \t\t\t\t(dt = 4.068 sec)\n",
      "[training epoch 0018]  16.5639 \t\t\t\t(dt = 4.128 sec)\n",
      "[training epoch 0019]  16.5934 \t\t\t\t(dt = 4.094 sec)\n",
      "[training epoch 0020]  16.6834 \t\t\t\t(dt = 4.178 sec)\n",
      "[training epoch 0021]  16.7419 \t\t\t\t(dt = 4.092 sec)\n",
      "[training epoch 0022]  16.7724 \t\t\t\t(dt = 4.175 sec)\n",
      "[training epoch 0023]  16.7728 \t\t\t\t(dt = 4.070 sec)\n",
      "[training epoch 0024]  16.8418 \t\t\t\t(dt = 4.071 sec)\n",
      "[training epoch 0025]  16.8561 \t\t\t\t(dt = 4.041 sec)\n",
      "[training epoch 0026]  16.8780 \t\t\t\t(dt = 4.170 sec)\n",
      "[training epoch 0027]  16.9217 \t\t\t\t(dt = 4.069 sec)\n",
      "[training epoch 0028]  17.0455 \t\t\t\t(dt = 4.050 sec)\n",
      "[training epoch 0029]  17.0954 \t\t\t\t(dt = 4.097 sec)\n",
      "[training epoch 0030]  17.0763 \t\t\t\t(dt = 4.157 sec)\n",
      "[training epoch 0031]  17.1037 \t\t\t\t(dt = 4.140 sec)\n",
      "[training epoch 0032]  17.1802 \t\t\t\t(dt = 4.236 sec)\n",
      "[training epoch 0033]  17.1910 \t\t\t\t(dt = 4.294 sec)\n",
      "[training epoch 0034]  17.2862 \t\t\t\t(dt = 4.086 sec)\n",
      "[training epoch 0035]  17.2958 \t\t\t\t(dt = 4.085 sec)\n",
      "[training epoch 0036]  17.3697 \t\t\t\t(dt = 4.072 sec)\n",
      "[training epoch 0037]  17.4044 \t\t\t\t(dt = 4.061 sec)\n",
      "[training epoch 0038]  17.4120 \t\t\t\t(dt = 4.214 sec)\n",
      "[training epoch 0039]  17.4834 \t\t\t\t(dt = 4.425 sec)\n",
      "[training epoch 0040]  17.5304 \t\t\t\t(dt = 3.998 sec)\n",
      "[training epoch 0041]  17.5997 \t\t\t\t(dt = 4.100 sec)\n",
      "[training epoch 0042]  17.6249 \t\t\t\t(dt = 4.083 sec)\n",
      "[training epoch 0043]  17.6313 \t\t\t\t(dt = 4.238 sec)\n",
      "[training epoch 0044]  17.6890 \t\t\t\t(dt = 4.253 sec)\n",
      "[training epoch 0045]  17.7229 \t\t\t\t(dt = 4.171 sec)\n",
      "[training epoch 0046]  17.7826 \t\t\t\t(dt = 4.123 sec)\n",
      "[training epoch 0047]  17.7958 \t\t\t\t(dt = 4.380 sec)\n",
      "[training epoch 0048]  17.8626 \t\t\t\t(dt = 4.070 sec)\n",
      "[training epoch 0049]  17.9063 \t\t\t\t(dt = 4.108 sec)\n",
      "[training epoch 0050]  17.9560 \t\t\t\t(dt = 4.055 sec)\n",
      "[val/test epoch 0050]  20.2707  19.3860\n",
      "[training epoch 0051]  17.9767 \t\t\t\t(dt = 4.691 sec)\n",
      "[training epoch 0052]  17.9980 \t\t\t\t(dt = 4.102 sec)\n",
      "[training epoch 0053]  18.0635 \t\t\t\t(dt = 4.074 sec)\n",
      "[training epoch 0054]  18.0657 \t\t\t\t(dt = 4.110 sec)\n",
      "[training epoch 0055]  18.1827 \t\t\t\t(dt = 4.163 sec)\n",
      "[training epoch 0056]  18.2204 \t\t\t\t(dt = 4.219 sec)\n",
      "[training epoch 0057]  18.2742 \t\t\t\t(dt = 4.267 sec)\n",
      "[training epoch 0058]  18.2413 \t\t\t\t(dt = 4.198 sec)\n",
      "[training epoch 0059]  18.3275 \t\t\t\t(dt = 4.232 sec)\n",
      "[training epoch 0060]  18.3952 \t\t\t\t(dt = 4.471 sec)\n",
      "[training epoch 0061]  18.3912 \t\t\t\t(dt = 4.326 sec)\n",
      "[training epoch 0062]  18.4441 \t\t\t\t(dt = 4.386 sec)\n",
      "[training epoch 0063]  18.4488 \t\t\t\t(dt = 4.314 sec)\n",
      "[training epoch 0064]  18.4860 \t\t\t\t(dt = 4.383 sec)\n",
      "[training epoch 0065]  18.5646 \t\t\t\t(dt = 4.179 sec)\n",
      "[training epoch 0066]  18.6094 \t\t\t\t(dt = 4.113 sec)\n",
      "[training epoch 0067]  18.6299 \t\t\t\t(dt = 4.198 sec)\n",
      "[training epoch 0068]  18.6833 \t\t\t\t(dt = 4.109 sec)\n",
      "[training epoch 0069]  18.7446 \t\t\t\t(dt = 4.080 sec)\n",
      "[training epoch 0070]  18.7695 \t\t\t\t(dt = 4.129 sec)\n",
      "[training epoch 0071]  18.8022 \t\t\t\t(dt = 4.080 sec)\n",
      "[training epoch 0072]  18.8525 \t\t\t\t(dt = 4.032 sec)\n",
      "[training epoch 0073]  18.9001 \t\t\t\t(dt = 4.194 sec)\n",
      "[training epoch 0074]  18.9447 \t\t\t\t(dt = 4.100 sec)\n",
      "[training epoch 0075]  19.0001 \t\t\t\t(dt = 4.107 sec)\n",
      "[training epoch 0076]  19.0061 \t\t\t\t(dt = 4.076 sec)\n",
      "[training epoch 0077]  19.0328 \t\t\t\t(dt = 4.126 sec)\n",
      "[training epoch 0078]  19.0690 \t\t\t\t(dt = 4.085 sec)\n",
      "[training epoch 0079]  19.1698 \t\t\t\t(dt = 4.117 sec)\n",
      "[training epoch 0080]  19.2346 \t\t\t\t(dt = 4.307 sec)\n",
      "[training epoch 0081]  19.2191 \t\t\t\t(dt = 4.591 sec)\n",
      "[training epoch 0082]  19.2599 \t\t\t\t(dt = 5.731 sec)\n",
      "[training epoch 0083]  19.3846 \t\t\t\t(dt = 5.238 sec)\n",
      "[training epoch 0084]  19.3195 \t\t\t\t(dt = 4.501 sec)\n",
      "[training epoch 0085]  19.4757 \t\t\t\t(dt = 4.572 sec)\n",
      "[training epoch 0086]  19.4496 \t\t\t\t(dt = 4.903 sec)\n",
      "[training epoch 0087]  19.4791 \t\t\t\t(dt = 5.639 sec)\n",
      "[training epoch 0088]  19.5043 \t\t\t\t(dt = 4.981 sec)\n",
      "[training epoch 0089]  19.5141 \t\t\t\t(dt = 6.670 sec)\n",
      "[training epoch 0090]  19.6757 \t\t\t\t(dt = 7.953 sec)\n",
      "[training epoch 0091]  19.7033 \t\t\t\t(dt = 7.979 sec)\n",
      "[training epoch 0092]  19.6982 \t\t\t\t(dt = 8.240 sec)\n",
      "[training epoch 0093]  19.6855 \t\t\t\t(dt = 5.998 sec)\n",
      "[training epoch 0094]  19.7591 \t\t\t\t(dt = 5.001 sec)\n",
      "[training epoch 0095]  19.8623 \t\t\t\t(dt = 4.537 sec)\n",
      "[training epoch 0096]  19.8208 \t\t\t\t(dt = 4.477 sec)\n",
      "[training epoch 0097]  19.8879 \t\t\t\t(dt = 4.570 sec)\n",
      "[training epoch 0098]  19.9679 \t\t\t\t(dt = 4.556 sec)\n",
      "[training epoch 0099]  19.9770 \t\t\t\t(dt = 4.559 sec)\n",
      "[training epoch 0100]  19.9780 \t\t\t\t(dt = 4.403 sec)\n",
      "[val/test epoch 0100]  20.4739  19.4675\n",
      "[training epoch 0101]  20.0494 \t\t\t\t(dt = 5.053 sec)\n",
      "[training epoch 0102]  20.0048 \t\t\t\t(dt = 4.459 sec)\n",
      "[training epoch 0103]  20.0360 \t\t\t\t(dt = 4.669 sec)\n",
      "[training epoch 0104]  20.0174 \t\t\t\t(dt = 4.812 sec)\n",
      "[training epoch 0105]  20.0110 \t\t\t\t(dt = 4.714 sec)\n",
      "[training epoch 0106]  20.0579 \t\t\t\t(dt = 4.787 sec)\n",
      "[training epoch 0107]  20.0523 \t\t\t\t(dt = 5.140 sec)\n",
      "[training epoch 0108]  19.9989 \t\t\t\t(dt = 4.526 sec)\n",
      "[training epoch 0109]  20.0462 \t\t\t\t(dt = 4.951 sec)\n",
      "[training epoch 0110]  19.9495 \t\t\t\t(dt = 4.708 sec)\n",
      "[training epoch 0111]  20.0230 \t\t\t\t(dt = 4.848 sec)\n",
      "[training epoch 0112]  19.9519 \t\t\t\t(dt = 4.774 sec)\n",
      "[training epoch 0113]  19.9799 \t\t\t\t(dt = 4.711 sec)\n",
      "[training epoch 0114]  19.9746 \t\t\t\t(dt = 4.590 sec)\n",
      "[training epoch 0115]  20.0119 \t\t\t\t(dt = 4.575 sec)\n",
      "[training epoch 0116]  19.9550 \t\t\t\t(dt = 4.670 sec)\n",
      "[training epoch 0117]  19.9771 \t\t\t\t(dt = 4.957 sec)\n",
      "[training epoch 0118]  20.0593 \t\t\t\t(dt = 4.780 sec)\n",
      "[training epoch 0119]  20.0374 \t\t\t\t(dt = 4.634 sec)\n",
      "[training epoch 0120]  20.1102 \t\t\t\t(dt = 4.747 sec)\n",
      "[training epoch 0121]  20.0048 \t\t\t\t(dt = 5.178 sec)\n",
      "[training epoch 0122]  20.0273 \t\t\t\t(dt = 4.903 sec)\n",
      "[training epoch 0123]  20.0196 \t\t\t\t(dt = 5.165 sec)\n",
      "[training epoch 0124]  19.9836 \t\t\t\t(dt = 4.729 sec)\n",
      "[training epoch 0125]  20.0373 \t\t\t\t(dt = 4.903 sec)\n",
      "[training epoch 0126]  20.0238 \t\t\t\t(dt = 5.222 sec)\n",
      "[training epoch 0127]  19.9446 \t\t\t\t(dt = 5.084 sec)\n",
      "[training epoch 0128]  20.0091 \t\t\t\t(dt = 5.139 sec)\n",
      "[training epoch 0129]  20.0121 \t\t\t\t(dt = 5.208 sec)\n",
      "[training epoch 0130]  19.9963 \t\t\t\t(dt = 5.064 sec)\n",
      "[training epoch 0131]  19.9715 \t\t\t\t(dt = 4.977 sec)\n",
      "[training epoch 0132]  20.0229 \t\t\t\t(dt = 4.974 sec)\n",
      "[training epoch 0133]  20.0613 \t\t\t\t(dt = 4.998 sec)\n",
      "[training epoch 0134]  20.0056 \t\t\t\t(dt = 4.954 sec)\n",
      "[training epoch 0135]  20.0605 \t\t\t\t(dt = 5.074 sec)\n",
      "[training epoch 0136]  20.0265 \t\t\t\t(dt = 5.038 sec)\n",
      "[training epoch 0137]  20.0497 \t\t\t\t(dt = 4.717 sec)\n",
      "[training epoch 0138]  20.0733 \t\t\t\t(dt = 4.792 sec)\n",
      "[training epoch 0139]  20.0006 \t\t\t\t(dt = 4.772 sec)\n",
      "[training epoch 0140]  20.0351 \t\t\t\t(dt = 5.093 sec)\n",
      "[training epoch 0141]  20.0013 \t\t\t\t(dt = 4.974 sec)\n",
      "[training epoch 0142]  20.0107 \t\t\t\t(dt = 5.103 sec)\n",
      "[training epoch 0143]  19.9513 \t\t\t\t(dt = 5.065 sec)\n",
      "[training epoch 0144]  20.0573 \t\t\t\t(dt = 5.150 sec)\n",
      "[training epoch 0145]  19.9653 \t\t\t\t(dt = 4.837 sec)\n",
      "[training epoch 0146]  19.9777 \t\t\t\t(dt = 4.853 sec)\n",
      "[training epoch 0147]  19.9866 \t\t\t\t(dt = 5.018 sec)\n",
      "[training epoch 0148]  20.0378 \t\t\t\t(dt = 4.820 sec)\n",
      "[training epoch 0149]  20.0434 \t\t\t\t(dt = 5.180 sec)\n",
      "[training epoch 0150]  20.0690 \t\t\t\t(dt = 4.796 sec)\n",
      "[val/test epoch 0150]  20.4116  19.3965\n",
      "[training epoch 0151]  19.9539 \t\t\t\t(dt = 6.233 sec)\n",
      "[training epoch 0152]  19.9643 \t\t\t\t(dt = 5.180 sec)\n",
      "[training epoch 0153]  19.9894 \t\t\t\t(dt = 4.857 sec)\n",
      "[training epoch 0154]  19.9689 \t\t\t\t(dt = 4.797 sec)\n",
      "[training epoch 0155]  20.0248 \t\t\t\t(dt = 4.838 sec)\n",
      "[training epoch 0156]  20.0432 \t\t\t\t(dt = 4.762 sec)\n",
      "[training epoch 0157]  19.9917 \t\t\t\t(dt = 4.823 sec)\n",
      "[training epoch 0158]  20.0033 \t\t\t\t(dt = 4.886 sec)\n",
      "[training epoch 0159]  19.9940 \t\t\t\t(dt = 5.610 sec)\n",
      "[training epoch 0160]  20.0017 \t\t\t\t(dt = 4.676 sec)\n",
      "[training epoch 0161]  19.9732 \t\t\t\t(dt = 5.563 sec)\n",
      "[training epoch 0162]  19.9631 \t\t\t\t(dt = 5.528 sec)\n",
      "[training epoch 0163]  20.0276 \t\t\t\t(dt = 4.758 sec)\n",
      "[training epoch 0164]  20.0136 \t\t\t\t(dt = 4.730 sec)\n",
      "[training epoch 0165]  19.9932 \t\t\t\t(dt = 4.603 sec)\n",
      "[training epoch 0166]  19.9441 \t\t\t\t(dt = 4.638 sec)\n",
      "[training epoch 0167]  19.9880 \t\t\t\t(dt = 5.010 sec)\n",
      "[training epoch 0168]  20.0583 \t\t\t\t(dt = 5.438 sec)\n",
      "[training epoch 0169]  20.0211 \t\t\t\t(dt = 4.746 sec)\n",
      "[training epoch 0170]  20.0289 \t\t\t\t(dt = 4.678 sec)\n",
      "[training epoch 0171]  19.9974 \t\t\t\t(dt = 4.679 sec)\n",
      "[training epoch 0172]  20.0141 \t\t\t\t(dt = 5.164 sec)\n",
      "[training epoch 0173]  19.9304 \t\t\t\t(dt = 4.840 sec)\n",
      "[training epoch 0174]  19.9946 \t\t\t\t(dt = 5.302 sec)\n",
      "[training epoch 0175]  20.0105 \t\t\t\t(dt = 4.965 sec)\n",
      "[training epoch 0176]  20.0291 \t\t\t\t(dt = 5.392 sec)\n",
      "[training epoch 0177]  20.0196 \t\t\t\t(dt = 5.016 sec)\n",
      "[training epoch 0178]  19.9603 \t\t\t\t(dt = 5.305 sec)\n",
      "[training epoch 0179]  20.0838 \t\t\t\t(dt = 5.359 sec)\n",
      "[training epoch 0180]  20.0531 \t\t\t\t(dt = 5.415 sec)\n",
      "[training epoch 0181]  20.0450 \t\t\t\t(dt = 5.275 sec)\n",
      "[training epoch 0182]  19.9891 \t\t\t\t(dt = 5.583 sec)\n",
      "[training epoch 0183]  20.0187 \t\t\t\t(dt = 5.617 sec)\n",
      "[training epoch 0184]  20.0043 \t\t\t\t(dt = 4.704 sec)\n",
      "[training epoch 0185]  20.0259 \t\t\t\t(dt = 5.082 sec)\n",
      "[training epoch 0186]  20.0286 \t\t\t\t(dt = 4.868 sec)\n",
      "[training epoch 0187]  19.9982 \t\t\t\t(dt = 4.671 sec)\n",
      "[training epoch 0188]  20.0104 \t\t\t\t(dt = 4.733 sec)\n",
      "[training epoch 0189]  19.9430 \t\t\t\t(dt = 4.753 sec)\n",
      "[training epoch 0190]  20.0042 \t\t\t\t(dt = 5.069 sec)\n",
      "[training epoch 0191]  20.0277 \t\t\t\t(dt = 5.212 sec)\n",
      "[training epoch 0192]  20.0206 \t\t\t\t(dt = 5.557 sec)\n",
      "[training epoch 0193]  20.0243 \t\t\t\t(dt = 4.938 sec)\n",
      "[training epoch 0194]  20.0377 \t\t\t\t(dt = 5.200 sec)\n",
      "[training epoch 0195]  20.0179 \t\t\t\t(dt = 4.858 sec)\n",
      "[training epoch 0196]  20.0368 \t\t\t\t(dt = 5.305 sec)\n",
      "[training epoch 0197]  19.9883 \t\t\t\t(dt = 4.813 sec)\n",
      "[training epoch 0198]  19.9618 \t\t\t\t(dt = 4.838 sec)\n",
      "[training epoch 0199]  20.0248 \t\t\t\t(dt = 5.181 sec)\n",
      "[training epoch 0200]  20.0052 \t\t\t\t(dt = 4.874 sec)\n",
      "[val/test epoch 0200]  20.3380  19.4879\n",
      "[training epoch 0201]  19.9712 \t\t\t\t(dt = 5.745 sec)\n",
      "[training epoch 0202]  20.0476 \t\t\t\t(dt = 5.144 sec)\n",
      "[training epoch 0203]  19.9934 \t\t\t\t(dt = 4.886 sec)\n",
      "[training epoch 0204]  20.0012 \t\t\t\t(dt = 4.841 sec)\n",
      "[training epoch 0205]  20.0385 \t\t\t\t(dt = 5.366 sec)\n",
      "[training epoch 0206]  19.9987 \t\t\t\t(dt = 5.260 sec)\n",
      "[training epoch 0207]  19.9939 \t\t\t\t(dt = 4.761 sec)\n",
      "[training epoch 0208]  20.0448 \t\t\t\t(dt = 4.982 sec)\n",
      "[training epoch 0209]  20.0479 \t\t\t\t(dt = 5.010 sec)\n",
      "[training epoch 0210]  20.0077 \t\t\t\t(dt = 5.028 sec)\n",
      "[training epoch 0211]  19.9777 \t\t\t\t(dt = 4.960 sec)\n",
      "[training epoch 0212]  19.9979 \t\t\t\t(dt = 5.216 sec)\n",
      "[training epoch 0213]  20.0408 \t\t\t\t(dt = 5.108 sec)\n",
      "[training epoch 0214]  20.0474 \t\t\t\t(dt = 4.911 sec)\n",
      "[training epoch 0215]  20.0445 \t\t\t\t(dt = 4.775 sec)\n",
      "[training epoch 0216]  20.0117 \t\t\t\t(dt = 5.497 sec)\n",
      "[training epoch 0217]  20.0127 \t\t\t\t(dt = 5.437 sec)\n",
      "[training epoch 0218]  19.9912 \t\t\t\t(dt = 4.998 sec)\n",
      "[training epoch 0219]  19.9892 \t\t\t\t(dt = 5.179 sec)\n",
      "[training epoch 0220]  19.9334 \t\t\t\t(dt = 5.391 sec)\n",
      "[training epoch 0221]  19.9821 \t\t\t\t(dt = 4.816 sec)\n",
      "[training epoch 0222]  19.9874 \t\t\t\t(dt = 5.700 sec)\n",
      "[training epoch 0223]  19.9948 \t\t\t\t(dt = 5.301 sec)\n",
      "[training epoch 0224]  20.0030 \t\t\t\t(dt = 5.629 sec)\n",
      "[training epoch 0225]  20.0034 \t\t\t\t(dt = 5.811 sec)\n",
      "[training epoch 0226]  20.0352 \t\t\t\t(dt = 4.947 sec)\n",
      "[training epoch 0227]  20.0531 \t\t\t\t(dt = 6.081 sec)\n",
      "[training epoch 0228]  20.0006 \t\t\t\t(dt = 5.477 sec)\n",
      "[training epoch 0229]  19.9814 \t\t\t\t(dt = 5.841 sec)\n",
      "[training epoch 0230]  19.9594 \t\t\t\t(dt = 4.961 sec)\n",
      "[training epoch 0231]  19.9928 \t\t\t\t(dt = 4.832 sec)\n",
      "[training epoch 0232]  19.9803 \t\t\t\t(dt = 4.875 sec)\n",
      "[training epoch 0233]  20.0088 \t\t\t\t(dt = 4.810 sec)\n",
      "[training epoch 0234]  20.0186 \t\t\t\t(dt = 5.000 sec)\n",
      "[training epoch 0235]  19.9606 \t\t\t\t(dt = 5.579 sec)\n",
      "[training epoch 0236]  19.9032 \t\t\t\t(dt = 4.911 sec)\n",
      "[training epoch 0237]  20.0238 \t\t\t\t(dt = 5.810 sec)\n",
      "[training epoch 0238]  19.9755 \t\t\t\t(dt = 5.373 sec)\n",
      "[training epoch 0239]  19.9869 \t\t\t\t(dt = 5.331 sec)\n",
      "[training epoch 0240]  19.9882 \t\t\t\t(dt = 5.011 sec)\n",
      "[training epoch 0241]  20.0148 \t\t\t\t(dt = 4.848 sec)\n",
      "[training epoch 0242]  20.0403 \t\t\t\t(dt = 4.985 sec)\n",
      "[training epoch 0243]  20.0267 \t\t\t\t(dt = 4.844 sec)\n",
      "[training epoch 0244]  20.0041 \t\t\t\t(dt = 4.915 sec)\n",
      "[training epoch 0245]  20.0561 \t\t\t\t(dt = 4.705 sec)\n",
      "[training epoch 0246]  19.9541 \t\t\t\t(dt = 4.732 sec)\n",
      "[training epoch 0247]  19.9939 \t\t\t\t(dt = 4.755 sec)\n",
      "[training epoch 0248]  20.0473 \t\t\t\t(dt = 4.668 sec)\n",
      "[training epoch 0249]  20.0085 \t\t\t\t(dt = 4.856 sec)\n",
      "[training epoch 0250]  19.9834 \t\t\t\t(dt = 4.834 sec)\n",
      "[val/test epoch 0250]  20.2832  19.4678\n",
      "[training epoch 0251]  19.9706 \t\t\t\t(dt = 5.503 sec)\n",
      "[training epoch 0252]  19.9965 \t\t\t\t(dt = 4.977 sec)\n",
      "[training epoch 0253]  20.0389 \t\t\t\t(dt = 4.888 sec)\n",
      "[training epoch 0254]  20.0456 \t\t\t\t(dt = 4.755 sec)\n",
      "[training epoch 0255]  19.9675 \t\t\t\t(dt = 4.769 sec)\n",
      "[training epoch 0256]  20.0250 \t\t\t\t(dt = 4.772 sec)\n",
      "[training epoch 0257]  20.0367 \t\t\t\t(dt = 4.656 sec)\n",
      "[training epoch 0258]  20.0729 \t\t\t\t(dt = 4.841 sec)\n",
      "[training epoch 0259]  20.0345 \t\t\t\t(dt = 5.050 sec)\n",
      "[training epoch 0260]  19.9947 \t\t\t\t(dt = 5.288 sec)\n",
      "[training epoch 0261]  19.9855 \t\t\t\t(dt = 5.331 sec)\n",
      "[training epoch 0262]  19.9655 \t\t\t\t(dt = 5.441 sec)\n",
      "[training epoch 0263]  19.9780 \t\t\t\t(dt = 5.214 sec)\n",
      "[training epoch 0264]  20.0220 \t\t\t\t(dt = 4.786 sec)\n",
      "[training epoch 0265]  20.0476 \t\t\t\t(dt = 4.918 sec)\n",
      "[training epoch 0266]  20.0595 \t\t\t\t(dt = 4.691 sec)\n",
      "[training epoch 0267]  20.0105 \t\t\t\t(dt = 4.811 sec)\n",
      "[training epoch 0268]  19.9276 \t\t\t\t(dt = 4.794 sec)\n",
      "[training epoch 0269]  19.9453 \t\t\t\t(dt = 4.804 sec)\n",
      "[training epoch 0270]  19.9720 \t\t\t\t(dt = 4.788 sec)\n",
      "[training epoch 0271]  20.0027 \t\t\t\t(dt = 4.796 sec)\n",
      "[training epoch 0272]  19.9967 \t\t\t\t(dt = 5.083 sec)\n",
      "[training epoch 0273]  20.0184 \t\t\t\t(dt = 4.955 sec)\n",
      "[training epoch 0274]  19.9582 \t\t\t\t(dt = 5.015 sec)\n",
      "[training epoch 0275]  19.9024 \t\t\t\t(dt = 5.283 sec)\n",
      "[training epoch 0276]  19.9881 \t\t\t\t(dt = 4.795 sec)\n",
      "[training epoch 0277]  20.0213 \t\t\t\t(dt = 5.113 sec)\n",
      "[training epoch 0278]  19.9206 \t\t\t\t(dt = 5.134 sec)\n",
      "[training epoch 0279]  20.0022 \t\t\t\t(dt = 5.109 sec)\n",
      "[training epoch 0280]  19.9736 \t\t\t\t(dt = 5.150 sec)\n",
      "[training epoch 0281]  20.0147 \t\t\t\t(dt = 5.122 sec)\n",
      "[training epoch 0282]  20.0483 \t\t\t\t(dt = 5.227 sec)\n",
      "[training epoch 0283]  19.9903 \t\t\t\t(dt = 5.067 sec)\n",
      "[training epoch 0284]  20.0382 \t\t\t\t(dt = 4.965 sec)\n",
      "[training epoch 0285]  20.0226 \t\t\t\t(dt = 5.067 sec)\n",
      "[training epoch 0286]  20.0032 \t\t\t\t(dt = 5.054 sec)\n",
      "[training epoch 0287]  20.0409 \t\t\t\t(dt = 5.465 sec)\n",
      "[training epoch 0288]  19.9719 \t\t\t\t(dt = 4.670 sec)\n",
      "[training epoch 0289]  19.9791 \t\t\t\t(dt = 4.911 sec)\n",
      "[training epoch 0290]  19.9447 \t\t\t\t(dt = 5.006 sec)\n",
      "[training epoch 0291]  20.0477 \t\t\t\t(dt = 5.197 sec)\n",
      "[training epoch 0292]  20.0136 \t\t\t\t(dt = 4.793 sec)\n",
      "[training epoch 0293]  20.0267 \t\t\t\t(dt = 4.806 sec)\n",
      "[training epoch 0294]  19.9920 \t\t\t\t(dt = 5.209 sec)\n",
      "[training epoch 0295]  20.0000 \t\t\t\t(dt = 5.284 sec)\n",
      "[training epoch 0296]  19.9683 \t\t\t\t(dt = 5.053 sec)\n",
      "[training epoch 0297]  20.1103 \t\t\t\t(dt = 5.011 sec)\n",
      "[training epoch 0298]  19.9944 \t\t\t\t(dt = 4.962 sec)\n",
      "[training epoch 0299]  19.9912 \t\t\t\t(dt = 4.978 sec)\n",
      "[training epoch 0300]  20.0231 \t\t\t\t(dt = 5.237 sec)\n",
      "[val/test epoch 0300]  20.2738  19.4092\n",
      "[training epoch 0301]  19.9414 \t\t\t\t(dt = 5.540 sec)\n",
      "[training epoch 0302]  20.0471 \t\t\t\t(dt = 4.797 sec)\n",
      "[training epoch 0303]  19.9480 \t\t\t\t(dt = 5.021 sec)\n",
      "[training epoch 0304]  20.0075 \t\t\t\t(dt = 5.281 sec)\n",
      "[training epoch 0305]  20.0162 \t\t\t\t(dt = 4.774 sec)\n",
      "[training epoch 0306]  19.9786 \t\t\t\t(dt = 5.174 sec)\n",
      "[training epoch 0307]  19.9706 \t\t\t\t(dt = 4.777 sec)\n",
      "[training epoch 0308]  20.0204 \t\t\t\t(dt = 5.284 sec)\n",
      "[training epoch 0309]  19.9934 \t\t\t\t(dt = 4.731 sec)\n",
      "[training epoch 0310]  20.0365 \t\t\t\t(dt = 5.204 sec)\n",
      "[training epoch 0311]  19.9845 \t\t\t\t(dt = 5.390 sec)\n",
      "[training epoch 0312]  19.9863 \t\t\t\t(dt = 5.217 sec)\n",
      "[training epoch 0313]  20.0562 \t\t\t\t(dt = 5.086 sec)\n",
      "[training epoch 0314]  20.0268 \t\t\t\t(dt = 5.458 sec)\n",
      "[training epoch 0315]  19.9724 \t\t\t\t(dt = 5.076 sec)\n",
      "[training epoch 0316]  20.0047 \t\t\t\t(dt = 5.242 sec)\n",
      "[training epoch 0317]  20.0059 \t\t\t\t(dt = 5.376 sec)\n",
      "[training epoch 0318]  20.0093 \t\t\t\t(dt = 4.758 sec)\n",
      "[training epoch 0319]  20.0245 \t\t\t\t(dt = 5.587 sec)\n",
      "[training epoch 0320]  20.0491 \t\t\t\t(dt = 5.213 sec)\n",
      "[training epoch 0321]  19.9786 \t\t\t\t(dt = 4.727 sec)\n",
      "[training epoch 0322]  19.9783 \t\t\t\t(dt = 5.074 sec)\n",
      "[training epoch 0323]  20.0639 \t\t\t\t(dt = 4.838 sec)\n",
      "[training epoch 0324]  19.9587 \t\t\t\t(dt = 5.328 sec)\n",
      "[training epoch 0325]  19.9902 \t\t\t\t(dt = 5.082 sec)\n",
      "[training epoch 0326]  20.0513 \t\t\t\t(dt = 4.856 sec)\n",
      "[training epoch 0327]  19.9793 \t\t\t\t(dt = 5.349 sec)\n",
      "[training epoch 0328]  19.9817 \t\t\t\t(dt = 5.232 sec)\n",
      "[training epoch 0329]  20.0006 \t\t\t\t(dt = 5.099 sec)\n",
      "[training epoch 0330]  19.9878 \t\t\t\t(dt = 5.505 sec)\n",
      "[training epoch 0331]  20.0373 \t\t\t\t(dt = 5.152 sec)\n",
      "[training epoch 0332]  20.0002 \t\t\t\t(dt = 5.073 sec)\n",
      "[training epoch 0333]  20.0335 \t\t\t\t(dt = 4.882 sec)\n",
      "[training epoch 0334]  20.0778 \t\t\t\t(dt = 4.766 sec)\n",
      "[training epoch 0335]  20.0331 \t\t\t\t(dt = 4.689 sec)\n",
      "[training epoch 0336]  19.9707 \t\t\t\t(dt = 4.709 sec)\n",
      "[training epoch 0337]  20.0389 \t\t\t\t(dt = 4.788 sec)\n",
      "[training epoch 0338]  19.9707 \t\t\t\t(dt = 4.687 sec)\n",
      "[training epoch 0339]  20.0242 \t\t\t\t(dt = 4.714 sec)\n",
      "[training epoch 0340]  19.9679 \t\t\t\t(dt = 5.764 sec)\n",
      "[training epoch 0341]  19.9916 \t\t\t\t(dt = 5.279 sec)\n",
      "[training epoch 0342]  19.9695 \t\t\t\t(dt = 5.569 sec)\n",
      "[training epoch 0343]  20.0045 \t\t\t\t(dt = 5.214 sec)\n",
      "[training epoch 0344]  19.9594 \t\t\t\t(dt = 5.462 sec)\n",
      "[training epoch 0345]  19.9793 \t\t\t\t(dt = 5.050 sec)\n",
      "[training epoch 0346]  19.9692 \t\t\t\t(dt = 5.629 sec)\n",
      "[training epoch 0347]  20.0504 \t\t\t\t(dt = 5.423 sec)\n",
      "[training epoch 0348]  20.0138 \t\t\t\t(dt = 5.645 sec)\n",
      "[training epoch 0349]  20.0143 \t\t\t\t(dt = 5.163 sec)\n",
      "[training epoch 0350]  19.9771 \t\t\t\t(dt = 5.351 sec)\n",
      "[val/test epoch 0350]  20.4077  19.4807\n",
      "[training epoch 0351]  20.0719 \t\t\t\t(dt = 6.180 sec)\n",
      "[training epoch 0352]  20.0128 \t\t\t\t(dt = 5.479 sec)\n",
      "[training epoch 0353]  20.0093 \t\t\t\t(dt = 5.399 sec)\n",
      "[training epoch 0354]  19.9547 \t\t\t\t(dt = 5.252 sec)\n",
      "[training epoch 0355]  19.9892 \t\t\t\t(dt = 4.803 sec)\n",
      "[training epoch 0356]  19.9993 \t\t\t\t(dt = 4.721 sec)\n",
      "[training epoch 0357]  19.9948 \t\t\t\t(dt = 4.743 sec)\n",
      "[training epoch 0358]  20.0202 \t\t\t\t(dt = 5.164 sec)\n",
      "[training epoch 0359]  20.0039 \t\t\t\t(dt = 4.994 sec)\n",
      "[training epoch 0360]  20.0018 \t\t\t\t(dt = 5.827 sec)\n",
      "[training epoch 0361]  19.9559 \t\t\t\t(dt = 5.007 sec)\n",
      "[training epoch 0362]  19.9768 \t\t\t\t(dt = 4.927 sec)\n",
      "[training epoch 0363]  20.0415 \t\t\t\t(dt = 5.039 sec)\n",
      "[training epoch 0364]  20.0395 \t\t\t\t(dt = 5.043 sec)\n",
      "[training epoch 0365]  19.9746 \t\t\t\t(dt = 5.205 sec)\n",
      "[training epoch 0366]  20.0222 \t\t\t\t(dt = 5.262 sec)\n",
      "[training epoch 0367]  19.9586 \t\t\t\t(dt = 4.963 sec)\n",
      "[training epoch 0368]  19.9630 \t\t\t\t(dt = 5.037 sec)\n",
      "[training epoch 0369]  20.0020 \t\t\t\t(dt = 5.364 sec)\n",
      "[training epoch 0370]  19.9860 \t\t\t\t(dt = 5.405 sec)\n",
      "[training epoch 0371]  20.0159 \t\t\t\t(dt = 5.003 sec)\n",
      "[training epoch 0372]  19.9948 \t\t\t\t(dt = 4.702 sec)\n",
      "[training epoch 0373]  20.0219 \t\t\t\t(dt = 5.270 sec)\n",
      "[training epoch 0374]  20.0026 \t\t\t\t(dt = 5.402 sec)\n",
      "[training epoch 0375]  19.9056 \t\t\t\t(dt = 4.854 sec)\n",
      "[training epoch 0376]  19.9897 \t\t\t\t(dt = 4.843 sec)\n",
      "[training epoch 0377]  19.9968 \t\t\t\t(dt = 4.813 sec)\n",
      "[training epoch 0378]  19.9874 \t\t\t\t(dt = 4.868 sec)\n",
      "[training epoch 0379]  20.0131 \t\t\t\t(dt = 4.835 sec)\n",
      "[training epoch 0380]  20.0298 \t\t\t\t(dt = 4.806 sec)\n",
      "[training epoch 0381]  19.9756 \t\t\t\t(dt = 4.893 sec)\n",
      "[training epoch 0382]  20.0106 \t\t\t\t(dt = 4.967 sec)\n",
      "[training epoch 0383]  20.0271 \t\t\t\t(dt = 4.843 sec)\n",
      "[training epoch 0384]  19.9993 \t\t\t\t(dt = 4.761 sec)\n",
      "[training epoch 0385]  20.0389 \t\t\t\t(dt = 4.794 sec)\n",
      "[training epoch 0386]  19.9882 \t\t\t\t(dt = 4.769 sec)\n",
      "[training epoch 0387]  19.9407 \t\t\t\t(dt = 4.917 sec)\n",
      "[training epoch 0388]  19.9973 \t\t\t\t(dt = 4.847 sec)\n",
      "[training epoch 0389]  20.0280 \t\t\t\t(dt = 4.856 sec)\n",
      "[training epoch 0390]  19.9926 \t\t\t\t(dt = 4.860 sec)\n",
      "[training epoch 0391]  20.0389 \t\t\t\t(dt = 5.004 sec)\n",
      "[training epoch 0392]  19.9723 \t\t\t\t(dt = 4.975 sec)\n",
      "[training epoch 0393]  20.0637 \t\t\t\t(dt = 5.536 sec)\n",
      "[training epoch 0394]  19.9561 \t\t\t\t(dt = 5.316 sec)\n",
      "[training epoch 0395]  20.0388 \t\t\t\t(dt = 5.295 sec)\n",
      "[training epoch 0396]  20.0872 \t\t\t\t(dt = 4.792 sec)\n",
      "[training epoch 0397]  20.0570 \t\t\t\t(dt = 5.029 sec)\n",
      "[training epoch 0398]  19.9698 \t\t\t\t(dt = 4.914 sec)\n",
      "[training epoch 0399]  19.9848 \t\t\t\t(dt = 4.890 sec)\n",
      "[training epoch 0400]  20.0261 \t\t\t\t(dt = 4.821 sec)\n",
      "[val/test epoch 0400]  20.1410  19.3607\n",
      "[training epoch 0401]  19.9477 \t\t\t\t(dt = 5.532 sec)\n",
      "[training epoch 0402]  20.0056 \t\t\t\t(dt = 4.708 sec)\n",
      "[training epoch 0403]  19.9635 \t\t\t\t(dt = 4.706 sec)\n",
      "[training epoch 0404]  20.0072 \t\t\t\t(dt = 4.931 sec)\n",
      "[training epoch 0405]  19.9818 \t\t\t\t(dt = 4.781 sec)\n",
      "[training epoch 0406]  20.0431 \t\t\t\t(dt = 5.129 sec)\n",
      "[training epoch 0407]  20.0802 \t\t\t\t(dt = 4.696 sec)\n",
      "[training epoch 0408]  19.9980 \t\t\t\t(dt = 4.619 sec)\n",
      "[training epoch 0409]  19.9289 \t\t\t\t(dt = 4.840 sec)\n",
      "[training epoch 0410]  19.9706 \t\t\t\t(dt = 5.641 sec)\n",
      "[training epoch 0411]  20.0014 \t\t\t\t(dt = 5.299 sec)\n",
      "[training epoch 0412]  19.9737 \t\t\t\t(dt = 5.048 sec)\n",
      "[training epoch 0413]  19.9973 \t\t\t\t(dt = 4.827 sec)\n",
      "[training epoch 0414]  20.0628 \t\t\t\t(dt = 4.962 sec)\n",
      "[training epoch 0415]  19.9855 \t\t\t\t(dt = 4.986 sec)\n",
      "[training epoch 0416]  20.0359 \t\t\t\t(dt = 5.293 sec)\n",
      "[training epoch 0417]  19.9752 \t\t\t\t(dt = 5.216 sec)\n",
      "[training epoch 0418]  19.9486 \t\t\t\t(dt = 5.168 sec)\n",
      "[training epoch 0419]  20.0367 \t\t\t\t(dt = 5.020 sec)\n",
      "[training epoch 0420]  19.9576 \t\t\t\t(dt = 5.259 sec)\n",
      "[training epoch 0421]  19.9395 \t\t\t\t(dt = 5.427 sec)\n",
      "[training epoch 0422]  19.9929 \t\t\t\t(dt = 6.231 sec)\n",
      "[training epoch 0423]  20.0580 \t\t\t\t(dt = 5.022 sec)\n",
      "[training epoch 0424]  19.9985 \t\t\t\t(dt = 4.879 sec)\n",
      "[training epoch 0425]  19.9924 \t\t\t\t(dt = 4.825 sec)\n",
      "[training epoch 0426]  19.9825 \t\t\t\t(dt = 4.895 sec)\n",
      "[training epoch 0427]  19.9985 \t\t\t\t(dt = 4.791 sec)\n",
      "[training epoch 0428]  20.0178 \t\t\t\t(dt = 5.529 sec)\n",
      "[training epoch 0429]  20.0523 \t\t\t\t(dt = 5.283 sec)\n",
      "[training epoch 0430]  20.0185 \t\t\t\t(dt = 5.207 sec)\n",
      "[training epoch 0431]  20.0678 \t\t\t\t(dt = 5.243 sec)\n",
      "[training epoch 0432]  20.0408 \t\t\t\t(dt = 4.951 sec)\n",
      "[training epoch 0433]  19.9991 \t\t\t\t(dt = 4.922 sec)\n",
      "[training epoch 0434]  20.0038 \t\t\t\t(dt = 4.655 sec)\n",
      "[training epoch 0435]  19.9621 \t\t\t\t(dt = 4.746 sec)\n",
      "[training epoch 0436]  20.0144 \t\t\t\t(dt = 4.982 sec)\n",
      "[training epoch 0437]  20.0738 \t\t\t\t(dt = 5.490 sec)\n",
      "[training epoch 0438]  19.9996 \t\t\t\t(dt = 5.258 sec)\n",
      "[training epoch 0439]  19.9941 \t\t\t\t(dt = 4.898 sec)\n",
      "[training epoch 0440]  19.9723 \t\t\t\t(dt = 4.936 sec)\n",
      "[training epoch 0441]  19.9903 \t\t\t\t(dt = 4.812 sec)\n",
      "[training epoch 0442]  20.0183 \t\t\t\t(dt = 4.765 sec)\n",
      "[training epoch 0443]  20.0188 \t\t\t\t(dt = 4.945 sec)\n",
      "[training epoch 0444]  20.0211 \t\t\t\t(dt = 5.000 sec)\n",
      "[training epoch 0445]  20.0343 \t\t\t\t(dt = 4.767 sec)\n",
      "[training epoch 0446]  20.1040 \t\t\t\t(dt = 4.743 sec)\n",
      "[training epoch 0447]  20.0570 \t\t\t\t(dt = 4.787 sec)\n",
      "[training epoch 0448]  20.0780 \t\t\t\t(dt = 4.893 sec)\n",
      "[training epoch 0449]  19.9217 \t\t\t\t(dt = 4.751 sec)\n",
      "[training epoch 0450]  20.0025 \t\t\t\t(dt = 4.701 sec)\n",
      "[val/test epoch 0450]  20.3926  19.3454\n",
      "[training epoch 0451]  19.9965 \t\t\t\t(dt = 5.432 sec)\n",
      "[training epoch 0452]  19.9275 \t\t\t\t(dt = 4.833 sec)\n",
      "[training epoch 0453]  20.0578 \t\t\t\t(dt = 4.734 sec)\n",
      "[training epoch 0454]  19.9902 \t\t\t\t(dt = 4.798 sec)\n",
      "[training epoch 0455]  20.0291 \t\t\t\t(dt = 4.763 sec)\n",
      "[training epoch 0456]  20.0826 \t\t\t\t(dt = 4.790 sec)\n",
      "[training epoch 0457]  19.9452 \t\t\t\t(dt = 4.791 sec)\n",
      "[training epoch 0458]  19.9900 \t\t\t\t(dt = 4.606 sec)\n",
      "[training epoch 0459]  19.9559 \t\t\t\t(dt = 4.752 sec)\n",
      "[training epoch 0460]  20.0455 \t\t\t\t(dt = 4.807 sec)\n",
      "[training epoch 0461]  19.9989 \t\t\t\t(dt = 4.733 sec)\n",
      "[training epoch 0462]  20.0378 \t\t\t\t(dt = 4.661 sec)\n",
      "[training epoch 0463]  19.9338 \t\t\t\t(dt = 4.698 sec)\n",
      "[training epoch 0464]  20.0479 \t\t\t\t(dt = 4.478 sec)\n",
      "[training epoch 0465]  19.9902 \t\t\t\t(dt = 4.456 sec)\n",
      "[training epoch 0466]  20.0504 \t\t\t\t(dt = 4.474 sec)\n",
      "[training epoch 0467]  20.0172 \t\t\t\t(dt = 4.497 sec)\n",
      "[training epoch 0468]  20.0253 \t\t\t\t(dt = 4.597 sec)\n",
      "[training epoch 0469]  20.0158 \t\t\t\t(dt = 4.491 sec)\n",
      "[training epoch 0470]  20.0119 \t\t\t\t(dt = 4.439 sec)\n",
      "[training epoch 0471]  19.9536 \t\t\t\t(dt = 4.528 sec)\n",
      "[training epoch 0472]  20.0250 \t\t\t\t(dt = 4.503 sec)\n",
      "[training epoch 0473]  20.0268 \t\t\t\t(dt = 4.543 sec)\n",
      "[training epoch 0474]  19.9227 \t\t\t\t(dt = 4.449 sec)\n",
      "[training epoch 0475]  19.9656 \t\t\t\t(dt = 4.444 sec)\n",
      "[training epoch 0476]  19.9465 \t\t\t\t(dt = 4.445 sec)\n",
      "[training epoch 0477]  19.9941 \t\t\t\t(dt = 5.050 sec)\n",
      "[training epoch 0478]  20.0121 \t\t\t\t(dt = 4.854 sec)\n",
      "[training epoch 0479]  19.9453 \t\t\t\t(dt = 4.770 sec)\n",
      "[training epoch 0480]  20.0540 \t\t\t\t(dt = 4.827 sec)\n",
      "[training epoch 0481]  20.0460 \t\t\t\t(dt = 4.763 sec)\n",
      "[training epoch 0482]  20.0351 \t\t\t\t(dt = 4.784 sec)\n",
      "[training epoch 0483]  20.0000 \t\t\t\t(dt = 4.629 sec)\n",
      "[training epoch 0484]  20.0213 \t\t\t\t(dt = 4.708 sec)\n",
      "[training epoch 0485]  19.9818 \t\t\t\t(dt = 4.703 sec)\n",
      "[training epoch 0486]  19.9655 \t\t\t\t(dt = 4.886 sec)\n",
      "[training epoch 0487]  19.9555 \t\t\t\t(dt = 4.899 sec)\n",
      "[training epoch 0488]  20.0207 \t\t\t\t(dt = 5.151 sec)\n",
      "[training epoch 0489]  20.0097 \t\t\t\t(dt = 5.065 sec)\n",
      "[training epoch 0490]  20.0870 \t\t\t\t(dt = 4.719 sec)\n",
      "[training epoch 0491]  19.9609 \t\t\t\t(dt = 4.636 sec)\n",
      "[training epoch 0492]  19.9613 \t\t\t\t(dt = 4.533 sec)\n",
      "[training epoch 0493]  20.0067 \t\t\t\t(dt = 4.482 sec)\n",
      "[training epoch 0494]  20.0025 \t\t\t\t(dt = 4.428 sec)\n",
      "[training epoch 0495]  20.0194 \t\t\t\t(dt = 4.460 sec)\n",
      "[training epoch 0496]  19.9887 \t\t\t\t(dt = 4.478 sec)\n",
      "[training epoch 0497]  19.9333 \t\t\t\t(dt = 4.649 sec)\n",
      "[training epoch 0498]  20.0114 \t\t\t\t(dt = 4.531 sec)\n",
      "[training epoch 0499]  19.9841 \t\t\t\t(dt = 4.641 sec)\n",
      "[training epoch 0500]  19.9892 \t\t\t\t(dt = 4.560 sec)\n",
      "[val/test epoch 0500]  20.3238  19.4404\n",
      "[training epoch 0501]  19.9819 \t\t\t\t(dt = 5.207 sec)\n",
      "[training epoch 0502]  20.0625 \t\t\t\t(dt = 5.108 sec)\n",
      "[training epoch 0503]  19.9347 \t\t\t\t(dt = 4.901 sec)\n",
      "[training epoch 0504]  20.0244 \t\t\t\t(dt = 4.915 sec)\n",
      "[training epoch 0505]  19.9984 \t\t\t\t(dt = 5.007 sec)\n",
      "[training epoch 0506]  20.0700 \t\t\t\t(dt = 5.009 sec)\n",
      "[training epoch 0507]  20.0170 \t\t\t\t(dt = 5.086 sec)\n",
      "[training epoch 0508]  19.9956 \t\t\t\t(dt = 4.962 sec)\n",
      "[training epoch 0509]  20.0471 \t\t\t\t(dt = 4.987 sec)\n",
      "[training epoch 0510]  19.9968 \t\t\t\t(dt = 4.659 sec)\n",
      "[training epoch 0511]  20.0111 \t\t\t\t(dt = 4.916 sec)\n",
      "[training epoch 0512]  20.0213 \t\t\t\t(dt = 5.103 sec)\n",
      "[training epoch 0513]  20.0916 \t\t\t\t(dt = 4.763 sec)\n",
      "[training epoch 0514]  20.0537 \t\t\t\t(dt = 4.888 sec)\n",
      "[training epoch 0515]  19.9799 \t\t\t\t(dt = 4.787 sec)\n",
      "[training epoch 0516]  19.9750 \t\t\t\t(dt = 4.740 sec)\n",
      "[training epoch 0517]  19.9978 \t\t\t\t(dt = 4.829 sec)\n",
      "[training epoch 0518]  20.0053 \t\t\t\t(dt = 4.827 sec)\n",
      "[training epoch 0519]  19.9939 \t\t\t\t(dt = 4.766 sec)\n",
      "[training epoch 0520]  20.0554 \t\t\t\t(dt = 4.742 sec)\n",
      "[training epoch 0521]  20.0751 \t\t\t\t(dt = 4.798 sec)\n",
      "[training epoch 0522]  19.9949 \t\t\t\t(dt = 4.752 sec)\n",
      "[training epoch 0523]  20.0795 \t\t\t\t(dt = 4.812 sec)\n",
      "[training epoch 0524]  19.9930 \t\t\t\t(dt = 4.774 sec)\n",
      "[training epoch 0525]  20.0198 \t\t\t\t(dt = 4.977 sec)\n",
      "[training epoch 0526]  19.9828 \t\t\t\t(dt = 4.997 sec)\n",
      "[training epoch 0527]  19.9906 \t\t\t\t(dt = 4.996 sec)\n",
      "[training epoch 0528]  20.0527 \t\t\t\t(dt = 4.794 sec)\n",
      "[training epoch 0529]  20.0589 \t\t\t\t(dt = 5.051 sec)\n",
      "[training epoch 0530]  20.0397 \t\t\t\t(dt = 5.001 sec)\n",
      "[training epoch 0531]  19.9461 \t\t\t\t(dt = 4.944 sec)\n",
      "[training epoch 0532]  20.0463 \t\t\t\t(dt = 4.998 sec)\n",
      "[training epoch 0533]  19.9582 \t\t\t\t(dt = 5.283 sec)\n",
      "[training epoch 0534]  20.0485 \t\t\t\t(dt = 5.160 sec)\n",
      "[training epoch 0535]  20.0129 \t\t\t\t(dt = 5.130 sec)\n",
      "[training epoch 0536]  20.0448 \t\t\t\t(dt = 5.376 sec)\n",
      "[training epoch 0537]  20.0048 \t\t\t\t(dt = 5.863 sec)\n",
      "[training epoch 0538]  19.9864 \t\t\t\t(dt = 5.472 sec)\n",
      "[training epoch 0539]  19.9921 \t\t\t\t(dt = 5.269 sec)\n",
      "[training epoch 0540]  19.9863 \t\t\t\t(dt = 5.275 sec)\n",
      "[training epoch 0541]  20.0045 \t\t\t\t(dt = 5.564 sec)\n",
      "[training epoch 0542]  19.9618 \t\t\t\t(dt = 5.757 sec)\n",
      "[training epoch 0543]  20.0232 \t\t\t\t(dt = 5.781 sec)\n",
      "[training epoch 0544]  20.0178 \t\t\t\t(dt = 5.340 sec)\n",
      "[training epoch 0545]  20.0195 \t\t\t\t(dt = 4.998 sec)\n",
      "[training epoch 0546]  19.9729 \t\t\t\t(dt = 4.758 sec)\n",
      "[training epoch 0547]  20.0489 \t\t\t\t(dt = 4.822 sec)\n",
      "[training epoch 0548]  19.9907 \t\t\t\t(dt = 4.833 sec)\n",
      "[training epoch 0549]  19.9836 \t\t\t\t(dt = 5.133 sec)\n",
      "[training epoch 0550]  20.0188 \t\t\t\t(dt = 5.296 sec)\n",
      "[val/test epoch 0550]  20.2470  19.4661\n",
      "[training epoch 0551]  19.9340 \t\t\t\t(dt = 5.660 sec)\n",
      "[training epoch 0552]  20.0552 \t\t\t\t(dt = 6.030 sec)\n",
      "[training epoch 0553]  20.0252 \t\t\t\t(dt = 5.262 sec)\n",
      "[training epoch 0554]  20.0181 \t\t\t\t(dt = 5.377 sec)\n",
      "[training epoch 0555]  19.9983 \t\t\t\t(dt = 5.169 sec)\n",
      "[training epoch 0556]  19.9274 \t\t\t\t(dt = 5.525 sec)\n",
      "[training epoch 0557]  19.9378 \t\t\t\t(dt = 5.921 sec)\n",
      "[training epoch 0558]  20.0579 \t\t\t\t(dt = 5.316 sec)\n",
      "[training epoch 0559]  19.9673 \t\t\t\t(dt = 5.645 sec)\n",
      "[training epoch 0560]  20.0223 \t\t\t\t(dt = 5.530 sec)\n",
      "[training epoch 0561]  20.0031 \t\t\t\t(dt = 4.920 sec)\n",
      "[training epoch 0562]  19.9866 \t\t\t\t(dt = 4.704 sec)\n",
      "[training epoch 0563]  19.9715 \t\t\t\t(dt = 5.488 sec)\n",
      "[training epoch 0564]  19.9852 \t\t\t\t(dt = 5.046 sec)\n",
      "[training epoch 0565]  19.9517 \t\t\t\t(dt = 5.168 sec)\n",
      "[training epoch 0566]  20.0401 \t\t\t\t(dt = 5.159 sec)\n",
      "[training epoch 0567]  19.9452 \t\t\t\t(dt = 5.422 sec)\n",
      "[training epoch 0568]  19.9800 \t\t\t\t(dt = 5.316 sec)\n",
      "[training epoch 0569]  20.1170 \t\t\t\t(dt = 5.530 sec)\n",
      "[training epoch 0570]  19.9561 \t\t\t\t(dt = 5.023 sec)\n",
      "[training epoch 0571]  20.0054 \t\t\t\t(dt = 4.906 sec)\n",
      "[training epoch 0572]  20.0415 \t\t\t\t(dt = 4.746 sec)\n",
      "[training epoch 0573]  20.0373 \t\t\t\t(dt = 4.743 sec)\n",
      "[training epoch 0574]  19.9571 \t\t\t\t(dt = 4.603 sec)\n",
      "[training epoch 0575]  20.0043 \t\t\t\t(dt = 4.688 sec)\n",
      "[training epoch 0576]  19.9704 \t\t\t\t(dt = 4.939 sec)\n",
      "[training epoch 0577]  20.0639 \t\t\t\t(dt = 4.852 sec)\n",
      "[training epoch 0578]  19.9943 \t\t\t\t(dt = 5.487 sec)\n",
      "[training epoch 0579]  19.9832 \t\t\t\t(dt = 4.768 sec)\n",
      "[training epoch 0580]  20.0143 \t\t\t\t(dt = 4.872 sec)\n",
      "[training epoch 0581]  20.0315 \t\t\t\t(dt = 4.738 sec)\n",
      "[training epoch 0582]  20.0890 \t\t\t\t(dt = 4.717 sec)\n",
      "[training epoch 0583]  19.9909 \t\t\t\t(dt = 4.595 sec)\n",
      "[training epoch 0584]  19.9736 \t\t\t\t(dt = 4.549 sec)\n",
      "[training epoch 0585]  19.9826 \t\t\t\t(dt = 4.541 sec)\n",
      "[training epoch 0586]  20.0239 \t\t\t\t(dt = 4.599 sec)\n",
      "[training epoch 0587]  19.9245 \t\t\t\t(dt = 4.604 sec)\n",
      "[training epoch 0588]  20.0140 \t\t\t\t(dt = 4.570 sec)\n",
      "[training epoch 0589]  20.0372 \t\t\t\t(dt = 4.549 sec)\n",
      "[training epoch 0590]  20.0389 \t\t\t\t(dt = 4.592 sec)\n",
      "[training epoch 0591]  20.0019 \t\t\t\t(dt = 4.588 sec)\n",
      "[training epoch 0592]  19.9897 \t\t\t\t(dt = 4.596 sec)\n",
      "[training epoch 0593]  20.0775 \t\t\t\t(dt = 4.963 sec)\n",
      "[training epoch 0594]  20.0044 \t\t\t\t(dt = 5.301 sec)\n",
      "[training epoch 0595]  20.0373 \t\t\t\t(dt = 4.733 sec)\n",
      "[training epoch 0596]  19.9810 \t\t\t\t(dt = 4.993 sec)\n",
      "[training epoch 0597]  20.0729 \t\t\t\t(dt = 4.895 sec)\n",
      "[training epoch 0598]  19.9866 \t\t\t\t(dt = 5.032 sec)\n",
      "[training epoch 0599]  19.9846 \t\t\t\t(dt = 5.484 sec)\n",
      "[training epoch 0600]  20.0139 \t\t\t\t(dt = 5.383 sec)\n",
      "[val/test epoch 0600]  20.5018  19.5893\n",
      "[training epoch 0601]  19.9655 \t\t\t\t(dt = 6.022 sec)\n",
      "[training epoch 0602]  20.1244 \t\t\t\t(dt = 5.686 sec)\n",
      "[training epoch 0603]  20.0338 \t\t\t\t(dt = 4.723 sec)\n",
      "[training epoch 0604]  20.0068 \t\t\t\t(dt = 5.334 sec)\n",
      "[training epoch 0605]  20.0007 \t\t\t\t(dt = 5.288 sec)\n",
      "[training epoch 0606]  19.9930 \t\t\t\t(dt = 5.191 sec)\n",
      "[training epoch 0607]  19.9972 \t\t\t\t(dt = 5.535 sec)\n",
      "[training epoch 0608]  20.0322 \t\t\t\t(dt = 4.445 sec)\n",
      "[training epoch 0609]  20.0406 \t\t\t\t(dt = 4.697 sec)\n",
      "[training epoch 0610]  20.0077 \t\t\t\t(dt = 4.888 sec)\n",
      "[training epoch 0611]  20.0121 \t\t\t\t(dt = 5.014 sec)\n",
      "[training epoch 0612]  20.0184 \t\t\t\t(dt = 5.061 sec)\n",
      "[training epoch 0613]  19.9640 \t\t\t\t(dt = 4.751 sec)\n",
      "[training epoch 0614]  20.0504 \t\t\t\t(dt = 4.710 sec)\n",
      "[training epoch 0615]  20.0015 \t\t\t\t(dt = 4.771 sec)\n",
      "[training epoch 0616]  20.0552 \t\t\t\t(dt = 4.946 sec)\n",
      "[training epoch 0617]  19.9794 \t\t\t\t(dt = 4.825 sec)\n",
      "[training epoch 0618]  20.0361 \t\t\t\t(dt = 5.570 sec)\n",
      "[training epoch 0619]  19.9944 \t\t\t\t(dt = 4.704 sec)\n",
      "[training epoch 0620]  20.0095 \t\t\t\t(dt = 4.577 sec)\n",
      "[training epoch 0621]  20.0230 \t\t\t\t(dt = 4.863 sec)\n",
      "[training epoch 0622]  19.9910 \t\t\t\t(dt = 4.886 sec)\n",
      "[training epoch 0623]  19.9758 \t\t\t\t(dt = 4.884 sec)\n",
      "[training epoch 0624]  19.9992 \t\t\t\t(dt = 4.770 sec)\n",
      "[training epoch 0625]  19.9866 \t\t\t\t(dt = 4.905 sec)\n",
      "[training epoch 0626]  20.0000 \t\t\t\t(dt = 4.730 sec)\n",
      "[training epoch 0627]  19.9830 \t\t\t\t(dt = 4.635 sec)\n",
      "[training epoch 0628]  20.0033 \t\t\t\t(dt = 4.644 sec)\n",
      "[training epoch 0629]  20.0105 \t\t\t\t(dt = 4.642 sec)\n",
      "[training epoch 0630]  20.0261 \t\t\t\t(dt = 4.910 sec)\n",
      "[training epoch 0631]  20.0050 \t\t\t\t(dt = 4.628 sec)\n",
      "[training epoch 0632]  20.0144 \t\t\t\t(dt = 4.655 sec)\n",
      "[training epoch 0633]  20.0545 \t\t\t\t(dt = 4.627 sec)\n",
      "[training epoch 0634]  19.9968 \t\t\t\t(dt = 4.650 sec)\n",
      "[training epoch 0635]  20.0038 \t\t\t\t(dt = 4.588 sec)\n",
      "[training epoch 0636]  19.9684 \t\t\t\t(dt = 4.621 sec)\n",
      "[training epoch 0637]  19.9907 \t\t\t\t(dt = 4.620 sec)\n",
      "[training epoch 0638]  19.9655 \t\t\t\t(dt = 4.597 sec)\n",
      "[training epoch 0639]  20.0272 \t\t\t\t(dt = 4.631 sec)\n",
      "[training epoch 0640]  19.9437 \t\t\t\t(dt = 4.646 sec)\n",
      "[training epoch 0641]  20.0807 \t\t\t\t(dt = 4.681 sec)\n",
      "[training epoch 0642]  19.9932 \t\t\t\t(dt = 4.798 sec)\n",
      "[training epoch 0643]  20.0158 \t\t\t\t(dt = 4.580 sec)\n",
      "[training epoch 0644]  19.9419 \t\t\t\t(dt = 4.639 sec)\n",
      "[training epoch 0645]  20.0179 \t\t\t\t(dt = 4.704 sec)\n",
      "[training epoch 0646]  19.9876 \t\t\t\t(dt = 4.911 sec)\n",
      "[training epoch 0647]  20.0158 \t\t\t\t(dt = 4.778 sec)\n",
      "[training epoch 0648]  20.0271 \t\t\t\t(dt = 4.594 sec)\n",
      "[training epoch 0649]  19.9984 \t\t\t\t(dt = 4.579 sec)\n",
      "[training epoch 0650]  20.0089 \t\t\t\t(dt = 4.570 sec)\n",
      "[val/test epoch 0650]  20.4928  19.7250\n",
      "[training epoch 0651]  19.9420 \t\t\t\t(dt = 5.231 sec)\n",
      "[training epoch 0652]  20.0196 \t\t\t\t(dt = 4.792 sec)\n",
      "[training epoch 0653]  20.0144 \t\t\t\t(dt = 4.718 sec)\n",
      "[training epoch 0654]  19.9845 \t\t\t\t(dt = 4.709 sec)\n",
      "[training epoch 0655]  20.0520 \t\t\t\t(dt = 4.643 sec)\n",
      "[training epoch 0656]  19.9688 \t\t\t\t(dt = 4.698 sec)\n",
      "[training epoch 0657]  19.9594 \t\t\t\t(dt = 4.857 sec)\n",
      "[training epoch 0658]  20.0276 \t\t\t\t(dt = 4.749 sec)\n",
      "[training epoch 0659]  20.0062 \t\t\t\t(dt = 4.708 sec)\n",
      "[training epoch 0660]  20.0424 \t\t\t\t(dt = 4.748 sec)\n",
      "[training epoch 0661]  19.9494 \t\t\t\t(dt = 4.705 sec)\n",
      "[training epoch 0662]  19.9795 \t\t\t\t(dt = 4.756 sec)\n",
      "[training epoch 0663]  19.9728 \t\t\t\t(dt = 5.714 sec)\n",
      "[training epoch 0664]  20.0172 \t\t\t\t(dt = 6.143 sec)\n",
      "[training epoch 0665]  19.9719 \t\t\t\t(dt = 6.060 sec)\n",
      "[training epoch 0666]  19.9943 \t\t\t\t(dt = 6.109 sec)\n",
      "[training epoch 0667]  20.0080 \t\t\t\t(dt = 5.697 sec)\n",
      "[training epoch 0668]  20.0079 \t\t\t\t(dt = 5.846 sec)\n",
      "[training epoch 0669]  19.9872 \t\t\t\t(dt = 5.271 sec)\n",
      "[training epoch 0670]  19.9891 \t\t\t\t(dt = 5.503 sec)\n",
      "[training epoch 0671]  19.9933 \t\t\t\t(dt = 5.514 sec)\n",
      "[training epoch 0672]  20.0156 \t\t\t\t(dt = 5.615 sec)\n",
      "[training epoch 0673]  19.9621 \t\t\t\t(dt = 5.731 sec)\n",
      "[training epoch 0674]  20.0488 \t\t\t\t(dt = 5.559 sec)\n",
      "[training epoch 0675]  20.0041 \t\t\t\t(dt = 5.564 sec)\n",
      "[training epoch 0676]  20.0345 \t\t\t\t(dt = 5.449 sec)\n",
      "[training epoch 0677]  20.0016 \t\t\t\t(dt = 5.445 sec)\n",
      "[training epoch 0678]  20.0740 \t\t\t\t(dt = 5.442 sec)\n",
      "[training epoch 0679]  20.0093 \t\t\t\t(dt = 5.416 sec)\n",
      "[training epoch 0680]  19.9638 \t\t\t\t(dt = 5.460 sec)\n",
      "[training epoch 0681]  19.9861 \t\t\t\t(dt = 5.501 sec)\n",
      "[training epoch 0682]  20.0136 \t\t\t\t(dt = 5.452 sec)\n",
      "[training epoch 0683]  20.0384 \t\t\t\t(dt = 5.459 sec)\n",
      "[training epoch 0684]  20.0258 \t\t\t\t(dt = 5.351 sec)\n",
      "[training epoch 0685]  20.0046 \t\t\t\t(dt = 5.467 sec)\n",
      "[training epoch 0686]  20.0030 \t\t\t\t(dt = 5.560 sec)\n",
      "[training epoch 0687]  19.9918 \t\t\t\t(dt = 5.615 sec)\n",
      "[training epoch 0688]  19.9839 \t\t\t\t(dt = 5.437 sec)\n",
      "[training epoch 0689]  20.0049 \t\t\t\t(dt = 5.613 sec)\n",
      "[training epoch 0690]  20.0239 \t\t\t\t(dt = 5.979 sec)\n",
      "[training epoch 0691]  19.9518 \t\t\t\t(dt = 5.799 sec)\n",
      "[training epoch 0692]  19.9866 \t\t\t\t(dt = 5.547 sec)\n",
      "[training epoch 0693]  19.9835 \t\t\t\t(dt = 5.469 sec)\n",
      "[training epoch 0694]  19.9824 \t\t\t\t(dt = 5.544 sec)\n",
      "[training epoch 0695]  20.0205 \t\t\t\t(dt = 5.641 sec)\n",
      "[training epoch 0696]  19.9330 \t\t\t\t(dt = 5.865 sec)\n",
      "[training epoch 0697]  20.0510 \t\t\t\t(dt = 5.569 sec)\n",
      "[training epoch 0698]  20.0105 \t\t\t\t(dt = 5.433 sec)\n",
      "[training epoch 0699]  20.0169 \t\t\t\t(dt = 5.290 sec)\n",
      "[training epoch 0700]  19.9947 \t\t\t\t(dt = 5.445 sec)\n",
      "[val/test epoch 0700]  20.3497  19.5695\n",
      "[training epoch 0701]  20.0386 \t\t\t\t(dt = 6.223 sec)\n",
      "[training epoch 0702]  20.0377 \t\t\t\t(dt = 5.471 sec)\n",
      "[training epoch 0703]  20.0474 \t\t\t\t(dt = 5.344 sec)\n",
      "[training epoch 0704]  20.0244 \t\t\t\t(dt = 5.382 sec)\n",
      "[training epoch 0705]  20.0267 \t\t\t\t(dt = 5.481 sec)\n",
      "[training epoch 0706]  20.0429 \t\t\t\t(dt = 5.358 sec)\n",
      "[training epoch 0707]  20.0345 \t\t\t\t(dt = 5.374 sec)\n",
      "[training epoch 0708]  19.9944 \t\t\t\t(dt = 5.374 sec)\n",
      "[training epoch 0709]  20.0644 \t\t\t\t(dt = 5.306 sec)\n",
      "[training epoch 0710]  19.9875 \t\t\t\t(dt = 5.379 sec)\n",
      "[training epoch 0711]  19.9818 \t\t\t\t(dt = 5.338 sec)\n",
      "[training epoch 0712]  20.0022 \t\t\t\t(dt = 5.341 sec)\n",
      "[training epoch 0713]  20.0473 \t\t\t\t(dt = 5.363 sec)\n",
      "[training epoch 0714]  20.0297 \t\t\t\t(dt = 5.251 sec)\n",
      "[training epoch 0715]  19.9927 \t\t\t\t(dt = 5.432 sec)\n",
      "[training epoch 0716]  20.0358 \t\t\t\t(dt = 5.531 sec)\n",
      "[training epoch 0717]  20.0605 \t\t\t\t(dt = 5.592 sec)\n",
      "[training epoch 0718]  20.0598 \t\t\t\t(dt = 5.918 sec)\n",
      "[training epoch 0719]  20.0253 \t\t\t\t(dt = 5.184 sec)\n",
      "[training epoch 0720]  20.0278 \t\t\t\t(dt = 4.951 sec)\n",
      "[training epoch 0721]  20.0165 \t\t\t\t(dt = 5.017 sec)\n",
      "[training epoch 0722]  20.0004 \t\t\t\t(dt = 4.887 sec)\n",
      "[training epoch 0723]  19.9916 \t\t\t\t(dt = 5.393 sec)\n",
      "[training epoch 0724]  20.0237 \t\t\t\t(dt = 5.736 sec)\n",
      "[training epoch 0725]  20.0462 \t\t\t\t(dt = 5.664 sec)\n",
      "[training epoch 0726]  19.9645 \t\t\t\t(dt = 6.295 sec)\n",
      "[training epoch 0727]  19.9851 \t\t\t\t(dt = 5.905 sec)\n",
      "[training epoch 0728]  19.9493 \t\t\t\t(dt = 5.116 sec)\n",
      "[training epoch 0729]  19.9858 \t\t\t\t(dt = 6.294 sec)\n",
      "[training epoch 0730]  19.9739 \t\t\t\t(dt = 5.712 sec)\n",
      "[training epoch 0731]  20.0067 \t\t\t\t(dt = 5.190 sec)\n",
      "[training epoch 0732]  19.9944 \t\t\t\t(dt = 4.807 sec)\n",
      "[training epoch 0733]  19.9769 \t\t\t\t(dt = 4.735 sec)\n",
      "[training epoch 0734]  20.0301 \t\t\t\t(dt = 4.776 sec)\n",
      "[training epoch 0735]  20.0268 \t\t\t\t(dt = 4.850 sec)\n",
      "[training epoch 0736]  20.0142 \t\t\t\t(dt = 4.831 sec)\n",
      "[training epoch 0737]  19.9794 \t\t\t\t(dt = 5.058 sec)\n",
      "[training epoch 0738]  20.0043 \t\t\t\t(dt = 4.701 sec)\n",
      "[training epoch 0739]  20.0090 \t\t\t\t(dt = 4.681 sec)\n",
      "[training epoch 0740]  20.0371 \t\t\t\t(dt = 4.864 sec)\n",
      "[training epoch 0741]  19.9466 \t\t\t\t(dt = 4.669 sec)\n",
      "[training epoch 0742]  20.0329 \t\t\t\t(dt = 4.730 sec)\n",
      "[training epoch 0743]  20.0045 \t\t\t\t(dt = 4.744 sec)\n",
      "[training epoch 0744]  19.9987 \t\t\t\t(dt = 4.750 sec)\n",
      "[training epoch 0745]  20.0365 \t\t\t\t(dt = 4.789 sec)\n",
      "[training epoch 0746]  19.9927 \t\t\t\t(dt = 4.634 sec)\n",
      "[training epoch 0747]  20.0052 \t\t\t\t(dt = 4.792 sec)\n",
      "[training epoch 0748]  19.9823 \t\t\t\t(dt = 4.737 sec)\n",
      "[training epoch 0749]  20.0864 \t\t\t\t(dt = 4.811 sec)\n",
      "[training epoch 0750]  19.9681 \t\t\t\t(dt = 4.718 sec)\n",
      "[val/test epoch 0750]  20.2147  19.4720\n",
      "[training epoch 0751]  19.9947 \t\t\t\t(dt = 5.383 sec)\n",
      "[training epoch 0752]  19.9614 \t\t\t\t(dt = 4.810 sec)\n",
      "[training epoch 0753]  19.9600 \t\t\t\t(dt = 4.739 sec)\n",
      "[training epoch 0754]  20.0037 \t\t\t\t(dt = 4.750 sec)\n",
      "[training epoch 0755]  20.0504 \t\t\t\t(dt = 4.715 sec)\n",
      "[training epoch 0756]  19.9634 \t\t\t\t(dt = 4.613 sec)\n",
      "[training epoch 0757]  19.9933 \t\t\t\t(dt = 4.707 sec)\n",
      "[training epoch 0758]  20.0017 \t\t\t\t(dt = 4.772 sec)\n",
      "[training epoch 0759]  19.9813 \t\t\t\t(dt = 4.711 sec)\n",
      "[training epoch 0760]  20.0140 \t\t\t\t(dt = 4.725 sec)\n",
      "[training epoch 0761]  20.0256 \t\t\t\t(dt = 4.547 sec)\n",
      "[training epoch 0762]  19.9787 \t\t\t\t(dt = 4.564 sec)\n",
      "[training epoch 0763]  19.9567 \t\t\t\t(dt = 4.731 sec)\n",
      "[training epoch 0764]  20.0061 \t\t\t\t(dt = 4.758 sec)\n",
      "[training epoch 0765]  19.9978 \t\t\t\t(dt = 4.575 sec)\n",
      "[training epoch 0766]  20.0171 \t\t\t\t(dt = 4.643 sec)\n",
      "[training epoch 0767]  20.0049 \t\t\t\t(dt = 4.801 sec)\n",
      "[training epoch 0768]  19.9890 \t\t\t\t(dt = 4.623 sec)\n",
      "[training epoch 0769]  20.0123 \t\t\t\t(dt = 4.751 sec)\n",
      "[training epoch 0770]  19.9861 \t\t\t\t(dt = 4.660 sec)\n",
      "[training epoch 0771]  19.9861 \t\t\t\t(dt = 4.606 sec)\n",
      "[training epoch 0772]  19.9839 \t\t\t\t(dt = 4.882 sec)\n",
      "[training epoch 0773]  19.9546 \t\t\t\t(dt = 5.556 sec)\n",
      "[training epoch 0774]  19.9927 \t\t\t\t(dt = 4.828 sec)\n",
      "[training epoch 0775]  20.0002 \t\t\t\t(dt = 4.842 sec)\n",
      "[training epoch 0776]  19.9617 \t\t\t\t(dt = 4.629 sec)\n",
      "[training epoch 0777]  19.9596 \t\t\t\t(dt = 4.909 sec)\n",
      "[training epoch 0778]  20.0001 \t\t\t\t(dt = 4.470 sec)\n",
      "[training epoch 0779]  20.0359 \t\t\t\t(dt = 4.451 sec)\n",
      "[training epoch 0780]  20.0130 \t\t\t\t(dt = 5.984 sec)\n",
      "[training epoch 0781]  19.9606 \t\t\t\t(dt = 5.397 sec)\n",
      "[training epoch 0782]  20.0211 \t\t\t\t(dt = 5.381 sec)\n",
      "[training epoch 0783]  20.0633 \t\t\t\t(dt = 4.599 sec)\n",
      "[training epoch 0784]  19.9744 \t\t\t\t(dt = 4.412 sec)\n",
      "[training epoch 0785]  19.9645 \t\t\t\t(dt = 4.564 sec)\n",
      "[training epoch 0786]  19.9776 \t\t\t\t(dt = 4.506 sec)\n",
      "[training epoch 0787]  20.0428 \t\t\t\t(dt = 4.469 sec)\n",
      "[training epoch 0788]  20.0199 \t\t\t\t(dt = 4.575 sec)\n",
      "[training epoch 0789]  20.0417 \t\t\t\t(dt = 4.714 sec)\n",
      "[training epoch 0790]  19.9745 \t\t\t\t(dt = 4.790 sec)\n",
      "[training epoch 0791]  20.0701 \t\t\t\t(dt = 5.271 sec)\n",
      "[training epoch 0792]  20.0321 \t\t\t\t(dt = 5.148 sec)\n",
      "[training epoch 0793]  20.0725 \t\t\t\t(dt = 5.258 sec)\n",
      "[training epoch 0794]  19.9652 \t\t\t\t(dt = 5.155 sec)\n",
      "[training epoch 0795]  20.0097 \t\t\t\t(dt = 4.422 sec)\n",
      "[training epoch 0796]  19.9904 \t\t\t\t(dt = 4.483 sec)\n",
      "[training epoch 0797]  20.0475 \t\t\t\t(dt = 4.449 sec)\n",
      "[training epoch 0798]  20.0310 \t\t\t\t(dt = 4.430 sec)\n",
      "[training epoch 0799]  19.9610 \t\t\t\t(dt = 4.397 sec)\n",
      "[training epoch 0800]  20.0607 \t\t\t\t(dt = 4.383 sec)\n",
      "[val/test epoch 0800]  20.2076  19.5834\n",
      "[training epoch 0801]  20.0657 \t\t\t\t(dt = 5.035 sec)\n",
      "[training epoch 0802]  20.0251 \t\t\t\t(dt = 4.525 sec)\n",
      "[training epoch 0803]  20.0098 \t\t\t\t(dt = 4.665 sec)\n",
      "[training epoch 0804]  19.9927 \t\t\t\t(dt = 4.763 sec)\n",
      "[training epoch 0805]  19.9253 \t\t\t\t(dt = 4.367 sec)\n",
      "[training epoch 0806]  19.9851 \t\t\t\t(dt = 4.490 sec)\n",
      "[training epoch 0807]  19.9845 \t\t\t\t(dt = 4.746 sec)\n",
      "[training epoch 0808]  20.0608 \t\t\t\t(dt = 4.801 sec)\n",
      "[training epoch 0809]  20.0267 \t\t\t\t(dt = 5.345 sec)\n",
      "[training epoch 0810]  19.9953 \t\t\t\t(dt = 4.978 sec)\n",
      "[training epoch 0811]  20.0048 \t\t\t\t(dt = 4.805 sec)\n",
      "[training epoch 0812]  19.9890 \t\t\t\t(dt = 5.215 sec)\n",
      "[training epoch 0813]  19.9706 \t\t\t\t(dt = 4.471 sec)\n",
      "[training epoch 0814]  20.0273 \t\t\t\t(dt = 4.405 sec)\n",
      "[training epoch 0815]  19.9882 \t\t\t\t(dt = 4.434 sec)\n",
      "[training epoch 0816]  19.9901 \t\t\t\t(dt = 4.394 sec)\n",
      "[training epoch 0817]  20.0390 \t\t\t\t(dt = 4.463 sec)\n",
      "[training epoch 0818]  20.0340 \t\t\t\t(dt = 4.506 sec)\n",
      "[training epoch 0819]  20.0336 \t\t\t\t(dt = 4.446 sec)\n",
      "[training epoch 0820]  19.9732 \t\t\t\t(dt = 4.433 sec)\n",
      "[training epoch 0821]  20.0579 \t\t\t\t(dt = 4.370 sec)\n",
      "[training epoch 0822]  20.0272 \t\t\t\t(dt = 4.380 sec)\n",
      "[training epoch 0823]  20.0017 \t\t\t\t(dt = 4.705 sec)\n",
      "[training epoch 0824]  20.0097 \t\t\t\t(dt = 4.408 sec)\n",
      "[training epoch 0825]  19.9960 \t\t\t\t(dt = 4.444 sec)\n",
      "[training epoch 0826]  19.9948 \t\t\t\t(dt = 4.465 sec)\n",
      "[training epoch 0827]  19.9260 \t\t\t\t(dt = 4.528 sec)\n",
      "[training epoch 0828]  20.0657 \t\t\t\t(dt = 4.555 sec)\n",
      "[training epoch 0829]  20.0441 \t\t\t\t(dt = 4.430 sec)\n",
      "[training epoch 0830]  20.0193 \t\t\t\t(dt = 4.422 sec)\n",
      "[training epoch 0831]  20.0121 \t\t\t\t(dt = 4.516 sec)\n",
      "[training epoch 0832]  19.9362 \t\t\t\t(dt = 4.492 sec)\n",
      "[training epoch 0833]  19.9621 \t\t\t\t(dt = 4.461 sec)\n",
      "[training epoch 0834]  20.0282 \t\t\t\t(dt = 4.391 sec)\n",
      "[training epoch 0835]  19.9832 \t\t\t\t(dt = 4.405 sec)\n",
      "[training epoch 0836]  20.0317 \t\t\t\t(dt = 4.443 sec)\n",
      "[training epoch 0837]  20.0735 \t\t\t\t(dt = 4.409 sec)\n",
      "[training epoch 0838]  19.9768 \t\t\t\t(dt = 4.394 sec)\n",
      "[training epoch 0839]  20.0565 \t\t\t\t(dt = 4.484 sec)\n",
      "[training epoch 0840]  20.0624 \t\t\t\t(dt = 4.434 sec)\n",
      "[training epoch 0841]  20.0594 \t\t\t\t(dt = 4.393 sec)\n",
      "[training epoch 0842]  20.0052 \t\t\t\t(dt = 4.421 sec)\n",
      "[training epoch 0843]  20.0059 \t\t\t\t(dt = 4.469 sec)\n",
      "[training epoch 0844]  20.0163 \t\t\t\t(dt = 4.418 sec)\n",
      "[training epoch 0845]  20.0129 \t\t\t\t(dt = 4.433 sec)\n",
      "[training epoch 0846]  19.9779 \t\t\t\t(dt = 4.384 sec)\n",
      "[training epoch 0847]  20.0359 \t\t\t\t(dt = 4.428 sec)\n",
      "[training epoch 0848]  19.9784 \t\t\t\t(dt = 4.409 sec)\n",
      "[training epoch 0849]  19.9812 \t\t\t\t(dt = 4.459 sec)\n",
      "[training epoch 0850]  20.0378 \t\t\t\t(dt = 4.400 sec)\n",
      "[val/test epoch 0850]  20.5994  19.7121\n",
      "[training epoch 0851]  19.9478 \t\t\t\t(dt = 5.079 sec)\n",
      "[training epoch 0852]  20.0008 \t\t\t\t(dt = 4.424 sec)\n",
      "[training epoch 0853]  19.9282 \t\t\t\t(dt = 4.443 sec)\n",
      "[training epoch 0854]  20.0417 \t\t\t\t(dt = 4.411 sec)\n",
      "[training epoch 0855]  19.9379 \t\t\t\t(dt = 4.465 sec)\n",
      "[training epoch 0856]  20.0485 \t\t\t\t(dt = 4.492 sec)\n",
      "[training epoch 0857]  20.0358 \t\t\t\t(dt = 4.436 sec)\n",
      "[training epoch 0858]  20.0254 \t\t\t\t(dt = 4.437 sec)\n",
      "[training epoch 0859]  19.9444 \t\t\t\t(dt = 4.446 sec)\n",
      "[training epoch 0860]  20.0599 \t\t\t\t(dt = 4.501 sec)\n",
      "[training epoch 0861]  19.9864 \t\t\t\t(dt = 4.400 sec)\n",
      "[training epoch 0862]  19.9685 \t\t\t\t(dt = 4.534 sec)\n",
      "[training epoch 0863]  20.0082 \t\t\t\t(dt = 4.430 sec)\n",
      "[training epoch 0864]  20.0294 \t\t\t\t(dt = 4.473 sec)\n",
      "[training epoch 0865]  20.0056 \t\t\t\t(dt = 4.385 sec)\n",
      "[training epoch 0866]  19.9719 \t\t\t\t(dt = 4.515 sec)\n",
      "[training epoch 0867]  19.9953 \t\t\t\t(dt = 4.495 sec)\n",
      "[training epoch 0868]  20.1058 \t\t\t\t(dt = 4.373 sec)\n",
      "[training epoch 0869]  19.9437 \t\t\t\t(dt = 4.392 sec)\n",
      "[training epoch 0870]  20.0118 \t\t\t\t(dt = 4.430 sec)\n",
      "[training epoch 0871]  19.9986 \t\t\t\t(dt = 4.385 sec)\n",
      "[training epoch 0872]  20.0366 \t\t\t\t(dt = 4.395 sec)\n",
      "[training epoch 0873]  20.0706 \t\t\t\t(dt = 4.428 sec)\n",
      "[training epoch 0874]  19.9863 \t\t\t\t(dt = 4.512 sec)\n",
      "[training epoch 0875]  20.0042 \t\t\t\t(dt = 4.415 sec)\n",
      "[training epoch 0876]  19.9740 \t\t\t\t(dt = 4.406 sec)\n",
      "[training epoch 0877]  19.9729 \t\t\t\t(dt = 4.354 sec)\n",
      "[training epoch 0878]  20.0131 \t\t\t\t(dt = 4.361 sec)\n",
      "[training epoch 0879]  20.0236 \t\t\t\t(dt = 4.425 sec)\n",
      "[training epoch 0880]  19.9763 \t\t\t\t(dt = 4.340 sec)\n",
      "[training epoch 0881]  19.9999 \t\t\t\t(dt = 4.379 sec)\n",
      "[training epoch 0882]  20.0143 \t\t\t\t(dt = 4.537 sec)\n",
      "[training epoch 0883]  19.9711 \t\t\t\t(dt = 4.377 sec)\n",
      "[training epoch 0884]  20.0161 \t\t\t\t(dt = 4.457 sec)\n",
      "[training epoch 0885]  20.0306 \t\t\t\t(dt = 4.380 sec)\n",
      "[training epoch 0886]  20.0337 \t\t\t\t(dt = 4.363 sec)\n",
      "[training epoch 0887]  20.0016 \t\t\t\t(dt = 4.404 sec)\n",
      "[training epoch 0888]  20.0208 \t\t\t\t(dt = 4.394 sec)\n",
      "[training epoch 0889]  20.0246 \t\t\t\t(dt = 4.455 sec)\n",
      "[training epoch 0890]  19.9858 \t\t\t\t(dt = 4.424 sec)\n",
      "[training epoch 0891]  19.9739 \t\t\t\t(dt = 4.377 sec)\n",
      "[training epoch 0892]  20.0640 \t\t\t\t(dt = 4.463 sec)\n",
      "[training epoch 0893]  19.9828 \t\t\t\t(dt = 4.382 sec)\n",
      "[training epoch 0894]  19.9946 \t\t\t\t(dt = 4.364 sec)\n",
      "[training epoch 0895]  20.0002 \t\t\t\t(dt = 4.401 sec)\n",
      "[training epoch 0896]  20.0101 \t\t\t\t(dt = 4.419 sec)\n",
      "[training epoch 0897]  19.9870 \t\t\t\t(dt = 4.449 sec)\n",
      "[training epoch 0898]  20.0354 \t\t\t\t(dt = 4.473 sec)\n",
      "[training epoch 0899]  19.9407 \t\t\t\t(dt = 4.349 sec)\n",
      "[training epoch 0900]  20.0122 \t\t\t\t(dt = 4.490 sec)\n",
      "[val/test epoch 0900]  20.2644  19.5727\n",
      "[training epoch 0901]  20.0291 \t\t\t\t(dt = 5.001 sec)\n",
      "[training epoch 0902]  19.9905 \t\t\t\t(dt = 4.415 sec)\n",
      "[training epoch 0903]  20.0772 \t\t\t\t(dt = 4.303 sec)\n",
      "[training epoch 0904]  19.9962 \t\t\t\t(dt = 4.342 sec)\n",
      "[training epoch 0905]  20.0500 \t\t\t\t(dt = 4.348 sec)\n",
      "[training epoch 0906]  20.0672 \t\t\t\t(dt = 4.396 sec)\n",
      "[training epoch 0907]  20.0501 \t\t\t\t(dt = 4.394 sec)\n",
      "[training epoch 0908]  20.0234 \t\t\t\t(dt = 4.331 sec)\n",
      "[training epoch 0909]  20.0035 \t\t\t\t(dt = 4.334 sec)\n",
      "[training epoch 0910]  20.0218 \t\t\t\t(dt = 4.446 sec)\n",
      "[training epoch 0911]  20.0086 \t\t\t\t(dt = 4.316 sec)\n",
      "[training epoch 0912]  20.0194 \t\t\t\t(dt = 4.417 sec)\n",
      "[training epoch 0913]  20.0612 \t\t\t\t(dt = 4.318 sec)\n",
      "[training epoch 0914]  20.0781 \t\t\t\t(dt = 4.320 sec)\n",
      "[training epoch 0915]  20.0004 \t\t\t\t(dt = 4.315 sec)\n",
      "[training epoch 0916]  20.0071 \t\t\t\t(dt = 4.395 sec)\n",
      "[training epoch 0917]  20.0312 \t\t\t\t(dt = 4.330 sec)\n",
      "[training epoch 0918]  20.0306 \t\t\t\t(dt = 4.373 sec)\n",
      "[training epoch 0919]  20.0111 \t\t\t\t(dt = 4.270 sec)\n",
      "[training epoch 0920]  20.0424 \t\t\t\t(dt = 4.356 sec)\n",
      "[training epoch 0921]  20.0234 \t\t\t\t(dt = 4.436 sec)\n",
      "[training epoch 0922]  20.0073 \t\t\t\t(dt = 4.276 sec)\n",
      "[training epoch 0923]  19.9769 \t\t\t\t(dt = 4.613 sec)\n",
      "[training epoch 0924]  20.0330 \t\t\t\t(dt = 4.330 sec)\n",
      "[training epoch 0925]  19.9928 \t\t\t\t(dt = 4.429 sec)\n",
      "[training epoch 0926]  20.0079 \t\t\t\t(dt = 4.371 sec)\n",
      "[training epoch 0927]  20.0117 \t\t\t\t(dt = 4.340 sec)\n",
      "[training epoch 0928]  20.0288 \t\t\t\t(dt = 4.376 sec)\n",
      "[training epoch 0929]  19.9794 \t\t\t\t(dt = 4.359 sec)\n",
      "[training epoch 0930]  20.0593 \t\t\t\t(dt = 4.318 sec)\n",
      "[training epoch 0931]  19.9425 \t\t\t\t(dt = 4.350 sec)\n",
      "[training epoch 0932]  19.9669 \t\t\t\t(dt = 4.405 sec)\n",
      "[training epoch 0933]  20.0087 \t\t\t\t(dt = 4.326 sec)\n",
      "[training epoch 0934]  19.9400 \t\t\t\t(dt = 4.297 sec)\n",
      "[training epoch 0935]  20.0240 \t\t\t\t(dt = 4.371 sec)\n",
      "[training epoch 0936]  20.0109 \t\t\t\t(dt = 4.284 sec)\n",
      "[training epoch 0937]  20.0360 \t\t\t\t(dt = 4.430 sec)\n",
      "[training epoch 0938]  20.0422 \t\t\t\t(dt = 4.404 sec)\n",
      "[training epoch 0939]  19.9972 \t\t\t\t(dt = 4.415 sec)\n",
      "[training epoch 0940]  19.9938 \t\t\t\t(dt = 4.466 sec)\n",
      "[training epoch 0941]  20.0618 \t\t\t\t(dt = 4.431 sec)\n",
      "[training epoch 0942]  19.9933 \t\t\t\t(dt = 4.437 sec)\n",
      "[training epoch 0943]  19.9881 \t\t\t\t(dt = 4.364 sec)\n",
      "[training epoch 0944]  20.0144 \t\t\t\t(dt = 4.412 sec)\n",
      "[training epoch 0945]  19.9715 \t\t\t\t(dt = 4.377 sec)\n",
      "[training epoch 0946]  20.0139 \t\t\t\t(dt = 4.611 sec)\n",
      "[training epoch 0947]  19.9948 \t\t\t\t(dt = 4.429 sec)\n",
      "[training epoch 0948]  19.9726 \t\t\t\t(dt = 4.344 sec)\n",
      "[training epoch 0949]  20.0385 \t\t\t\t(dt = 4.378 sec)\n",
      "[training epoch 0950]  19.9597 \t\t\t\t(dt = 4.429 sec)\n",
      "[val/test epoch 0950]  20.3841  19.5344\n",
      "[training epoch 0951]  19.9958 \t\t\t\t(dt = 5.114 sec)\n",
      "[training epoch 0952]  20.0005 \t\t\t\t(dt = 4.469 sec)\n",
      "[training epoch 0953]  20.0258 \t\t\t\t(dt = 4.457 sec)\n",
      "[training epoch 0954]  19.9916 \t\t\t\t(dt = 4.365 sec)\n",
      "[training epoch 0955]  20.0513 \t\t\t\t(dt = 4.430 sec)\n",
      "[training epoch 0956]  20.0033 \t\t\t\t(dt = 4.426 sec)\n",
      "[training epoch 0957]  20.0084 \t\t\t\t(dt = 4.415 sec)\n",
      "[training epoch 0958]  19.9543 \t\t\t\t(dt = 4.415 sec)\n",
      "[training epoch 0959]  20.0059 \t\t\t\t(dt = 4.391 sec)\n",
      "[training epoch 0960]  20.0443 \t\t\t\t(dt = 4.423 sec)\n",
      "[training epoch 0961]  20.0542 \t\t\t\t(dt = 4.442 sec)\n",
      "[training epoch 0962]  20.0327 \t\t\t\t(dt = 4.331 sec)\n",
      "[training epoch 0963]  20.0224 \t\t\t\t(dt = 4.424 sec)\n",
      "[training epoch 0964]  20.0746 \t\t\t\t(dt = 4.308 sec)\n",
      "[training epoch 0965]  19.9722 \t\t\t\t(dt = 4.439 sec)\n",
      "[training epoch 0966]  20.0603 \t\t\t\t(dt = 4.411 sec)\n",
      "[training epoch 0967]  20.0126 \t\t\t\t(dt = 4.472 sec)\n",
      "[training epoch 0968]  20.0308 \t\t\t\t(dt = 4.425 sec)\n",
      "[training epoch 0969]  20.0868 \t\t\t\t(dt = 4.426 sec)\n",
      "[training epoch 0970]  20.0289 \t\t\t\t(dt = 4.456 sec)\n",
      "[training epoch 0971]  19.9861 \t\t\t\t(dt = 4.431 sec)\n",
      "[training epoch 0972]  20.0336 \t\t\t\t(dt = 4.470 sec)\n",
      "[training epoch 0973]  19.9906 \t\t\t\t(dt = 4.418 sec)\n",
      "[training epoch 0974]  19.9819 \t\t\t\t(dt = 4.361 sec)\n",
      "[training epoch 0975]  20.0621 \t\t\t\t(dt = 4.481 sec)\n",
      "[training epoch 0976]  20.0302 \t\t\t\t(dt = 4.416 sec)\n",
      "[training epoch 0977]  19.9993 \t\t\t\t(dt = 4.375 sec)\n",
      "[training epoch 0978]  20.0028 \t\t\t\t(dt = 4.421 sec)\n",
      "[training epoch 0979]  19.9842 \t\t\t\t(dt = 4.362 sec)\n",
      "[training epoch 0980]  20.0080 \t\t\t\t(dt = 4.361 sec)\n",
      "[training epoch 0981]  19.9426 \t\t\t\t(dt = 4.435 sec)\n",
      "[training epoch 0982]  19.9844 \t\t\t\t(dt = 4.585 sec)\n",
      "[training epoch 0983]  19.9981 \t\t\t\t(dt = 4.414 sec)\n",
      "[training epoch 0984]  20.0012 \t\t\t\t(dt = 4.451 sec)\n",
      "[training epoch 0985]  20.0421 \t\t\t\t(dt = 4.383 sec)\n",
      "[training epoch 0986]  19.9415 \t\t\t\t(dt = 4.423 sec)\n",
      "[training epoch 0987]  20.0653 \t\t\t\t(dt = 4.403 sec)\n",
      "[training epoch 0988]  19.9820 \t\t\t\t(dt = 4.444 sec)\n",
      "[training epoch 0989]  20.0528 \t\t\t\t(dt = 4.419 sec)\n",
      "[training epoch 0990]  20.0431 \t\t\t\t(dt = 4.484 sec)\n",
      "[training epoch 0991]  20.0188 \t\t\t\t(dt = 4.406 sec)\n",
      "[training epoch 0992]  20.0158 \t\t\t\t(dt = 4.601 sec)\n",
      "[training epoch 0993]  19.9859 \t\t\t\t(dt = 4.387 sec)\n",
      "[training epoch 0994]  20.0228 \t\t\t\t(dt = 4.371 sec)\n",
      "[training epoch 0995]  19.9708 \t\t\t\t(dt = 4.429 sec)\n",
      "[training epoch 0996]  20.0319 \t\t\t\t(dt = 4.413 sec)\n",
      "[training epoch 0997]  19.9661 \t\t\t\t(dt = 4.437 sec)\n",
      "[training epoch 0998]  20.0205 \t\t\t\t(dt = 4.406 sec)\n",
      "[training epoch 0999]  20.0156 \t\t\t\t(dt = 4.387 sec)\n",
      "[training epoch 1000]  20.0109 \t\t\t\t(dt = 4.565 sec)\n",
      "[val/test epoch 1000]  20.2906  19.4806\n",
      "[training epoch 1001]  20.0096 \t\t\t\t(dt = 5.113 sec)\n",
      "[training epoch 1002]  20.0998 \t\t\t\t(dt = 4.414 sec)\n",
      "[training epoch 1003]  20.0615 \t\t\t\t(dt = 4.436 sec)\n",
      "[training epoch 1004]  20.0093 \t\t\t\t(dt = 4.496 sec)\n",
      "[training epoch 1005]  19.9889 \t\t\t\t(dt = 4.359 sec)\n",
      "[training epoch 1006]  20.0336 \t\t\t\t(dt = 4.362 sec)\n",
      "[training epoch 1007]  20.0902 \t\t\t\t(dt = 4.379 sec)\n",
      "[training epoch 1008]  19.9899 \t\t\t\t(dt = 4.391 sec)\n",
      "[training epoch 1009]  20.0294 \t\t\t\t(dt = 4.383 sec)\n",
      "[training epoch 1010]  19.9861 \t\t\t\t(dt = 4.402 sec)\n",
      "[training epoch 1011]  19.9779 \t\t\t\t(dt = 4.415 sec)\n",
      "[training epoch 1012]  20.0190 \t\t\t\t(dt = 5.105 sec)\n",
      "[training epoch 1013]  20.0198 \t\t\t\t(dt = 4.734 sec)\n",
      "[training epoch 1014]  20.0267 \t\t\t\t(dt = 5.059 sec)\n",
      "[training epoch 1015]  19.9637 \t\t\t\t(dt = 4.644 sec)\n",
      "[training epoch 1016]  20.0291 \t\t\t\t(dt = 4.564 sec)\n",
      "[training epoch 1017]  20.0549 \t\t\t\t(dt = 4.529 sec)\n",
      "[training epoch 1018]  19.9845 \t\t\t\t(dt = 4.553 sec)\n",
      "[training epoch 1019]  20.0534 \t\t\t\t(dt = 4.659 sec)\n",
      "[training epoch 1020]  20.0100 \t\t\t\t(dt = 4.538 sec)\n",
      "[training epoch 1021]  20.0820 \t\t\t\t(dt = 4.571 sec)\n",
      "[training epoch 1022]  20.0228 \t\t\t\t(dt = 4.604 sec)\n",
      "[training epoch 1023]  19.9861 \t\t\t\t(dt = 4.884 sec)\n",
      "[training epoch 1024]  20.0273 \t\t\t\t(dt = 4.756 sec)\n",
      "[training epoch 1025]  19.9985 \t\t\t\t(dt = 4.305 sec)\n",
      "[training epoch 1026]  19.9770 \t\t\t\t(dt = 4.346 sec)\n",
      "[training epoch 1027]  19.9788 \t\t\t\t(dt = 4.444 sec)\n",
      "[training epoch 1028]  20.0013 \t\t\t\t(dt = 4.275 sec)\n",
      "[training epoch 1029]  19.9711 \t\t\t\t(dt = 4.181 sec)\n",
      "[training epoch 1030]  20.0328 \t\t\t\t(dt = 4.198 sec)\n",
      "[training epoch 1031]  20.0285 \t\t\t\t(dt = 4.209 sec)\n",
      "[training epoch 1032]  20.0035 \t\t\t\t(dt = 4.272 sec)\n",
      "[training epoch 1033]  19.9751 \t\t\t\t(dt = 4.217 sec)\n",
      "[training epoch 1034]  20.0553 \t\t\t\t(dt = 4.343 sec)\n",
      "[training epoch 1035]  19.9114 \t\t\t\t(dt = 4.264 sec)\n",
      "[training epoch 1036]  19.9971 \t\t\t\t(dt = 4.228 sec)\n",
      "[training epoch 1037]  20.0478 \t\t\t\t(dt = 4.267 sec)\n",
      "[training epoch 1038]  20.0888 \t\t\t\t(dt = 4.257 sec)\n",
      "[training epoch 1039]  20.0546 \t\t\t\t(dt = 4.224 sec)\n",
      "[training epoch 1040]  19.9211 \t\t\t\t(dt = 4.228 sec)\n",
      "[training epoch 1041]  19.9781 \t\t\t\t(dt = 4.195 sec)\n",
      "[training epoch 1042]  19.9910 \t\t\t\t(dt = 4.209 sec)\n",
      "[training epoch 1043]  19.9946 \t\t\t\t(dt = 4.216 sec)\n",
      "[training epoch 1044]  19.9802 \t\t\t\t(dt = 4.217 sec)\n",
      "[training epoch 1045]  19.9737 \t\t\t\t(dt = 4.209 sec)\n",
      "[training epoch 1046]  20.0103 \t\t\t\t(dt = 4.216 sec)\n",
      "[training epoch 1047]  20.0659 \t\t\t\t(dt = 4.254 sec)\n",
      "[training epoch 1048]  19.9483 \t\t\t\t(dt = 4.211 sec)\n",
      "[training epoch 1049]  19.9745 \t\t\t\t(dt = 4.209 sec)\n",
      "[training epoch 1050]  20.0129 \t\t\t\t(dt = 4.202 sec)\n",
      "[val/test epoch 1050]  20.2522  19.4553\n",
      "[training epoch 1051]  20.0128 \t\t\t\t(dt = 4.864 sec)\n",
      "[training epoch 1052]  20.0557 \t\t\t\t(dt = 4.233 sec)\n",
      "[training epoch 1053]  20.0109 \t\t\t\t(dt = 4.173 sec)\n",
      "[training epoch 1054]  19.9869 \t\t\t\t(dt = 4.215 sec)\n",
      "[training epoch 1055]  20.0301 \t\t\t\t(dt = 4.205 sec)\n",
      "[training epoch 1056]  20.0196 \t\t\t\t(dt = 4.166 sec)\n",
      "[training epoch 1057]  20.0344 \t\t\t\t(dt = 4.442 sec)\n",
      "[training epoch 1058]  19.9487 \t\t\t\t(dt = 4.312 sec)\n",
      "[training epoch 1059]  20.0114 \t\t\t\t(dt = 4.466 sec)\n",
      "[training epoch 1060]  19.9835 \t\t\t\t(dt = 4.027 sec)\n",
      "[training epoch 1061]  19.9709 \t\t\t\t(dt = 4.003 sec)\n",
      "[training epoch 1062]  20.0480 \t\t\t\t(dt = 4.060 sec)\n",
      "[training epoch 1063]  19.9946 \t\t\t\t(dt = 3.960 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-df856d0ea826>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# process each mini-batch; this is where we take gradient steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mwhich_mini_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_mini_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mepoch_nll\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprocess_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich_mini_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffled_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# report training diagnostics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-c6a797042d72>\u001b[0m in \u001b[0;36mprocess_minibatch\u001b[1;34m(epoch, which_mini_batch, shuffled_indices, mini_batch_size, annealing_epochs, minimum_annealing_factor)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# do an actual gradient step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     loss = svi.step(mini_batch, mini_batch_reversed, mini_batch_mask,\n\u001b[0m\u001b[0;32m     29\u001b[0m                      mini_batch_seq_lengths, annealing_factor)\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# keep track of the training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pyro\\lib\\site-packages\\pyro\\infer\\svi.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;31m# get loss and compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         params = set(\n",
      "\u001b[1;32m~\\.conda\\envs\\pyro\\lib\\site-packages\\pyro\\infer\\trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m             ):\n\u001b[0;32m    156\u001b[0m                 \u001b[0msurrogate_loss_particle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0msurrogate_loss_particle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[0mwarn_if_nan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pyro\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pyro\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "times = [time.time()]\n",
    "val_nll_list = list()\n",
    "for epoch in range(num_epochs):\n",
    "    # accumulator for our estimate of the negative log likelihood\n",
    "    # (or rather -elbo) for this epoch\n",
    "    epoch_nll = 0.0\n",
    "    # prepare mini-batch subsampling indices for this epoch\n",
    "    shuffled_indices = np.arange(N_train_data)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "\n",
    "    # process each mini-batch; this is where we take gradient steps\n",
    "    for which_mini_batch in range(N_mini_batches):\n",
    "        epoch_nll += process_minibatch(epoch, which_mini_batch, shuffled_indices, 10, 100, .01)\n",
    "\n",
    "    # report training diagnostics\n",
    "    times.append(time.time())\n",
    "    epoch_time = times[-1] - times[-2]\n",
    "    print(\"[training epoch %04d]  %.4f \\t\\t\\t\\t(dt = %.3f sec)\" %\n",
    "        (epoch, epoch_nll / N_train_time_slices, epoch_time))\n",
    "    \n",
    "    # do evaluation on test and validation data and report results\n",
    "    if val_test_frequency > 0 and epoch > 0 and epoch % val_test_frequency == 0:\n",
    "        val_nll, test_nll = do_evaluation()\n",
    "        val_nll_list.append(val_nll)\n",
    "        print(\n",
    "            \"[val/test epoch %04d]  %.4f  %.4f\" % (epoch, val_nll, test_nll)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
